import pandas as pd
import json
import re
import dgl
import numpy as np
import torch
import torch.nn.functional as F
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Union, Optional
import nltk
from nltk.corpus import stopwords


# å¯¼å…¥è®­ç»ƒé˜¶æ®µå®šä¹‰çš„HANæ¨¡å‹ï¼ˆå¿…é¡»ä¸è®­ç»ƒä»£ç ä¸­çš„æ¨¡å‹å®šä¹‰å®Œå…¨ä¸€è‡´ï¼‰
class HANModel(torch.nn.Module):
    def __init__(self,
                 g: dgl.DGLHeteroGraph,
                 in_feats_dict: Dict[str, int],
                 embed_dim: int,
                 num_heads: int,
                 hidden_dim: int = 32):
        super().__init__()
        self.g = g
        self.node_types = g.ntypes
        self.canonical_etypes = g.canonical_etypes
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.hidden_dim = hidden_dim

        # ç¬¬ä¸€å±‚GAT + HeteroGraphConv
        self.gat1_conv_dict = {}
        for src_ntype, etype, dst_ntype in self.canonical_etypes:
            self.gat1_conv_dict[etype] = dgl.nn.GATConv(
                in_feats=(in_feats_dict[src_ntype], in_feats_dict[dst_ntype]),
                out_feats=self.hidden_dim,
                num_heads=self.num_heads,
                allow_zero_in_degree=True
            )
        self.hetero_conv1 = dgl.nn.HeteroGraphConv(
            mods=self.gat1_conv_dict,
            aggregate="sum"
        )

        # ç¬¬äºŒå±‚GAT + HeteroGraphConv
        self.gat2_conv_dict = {}
        for src_ntype, etype, dst_ntype in self.canonical_etypes:
            in_feat_src = self.hidden_dim * self.num_heads
            in_feat_dst = self.hidden_dim * self.num_heads
            self.gat2_conv_dict[etype] = dgl.nn.GATConv(
                in_feats=(in_feat_src, in_feat_dst),
                out_feats=self.embed_dim,
                num_heads=1,
                allow_zero_in_degree=True
            )
        self.hetero_conv2 = dgl.nn.HeteroGraphConv(
            mods=self.gat2_conv_dict,
            aggregate="sum"
        )

        self.semantic_attn1 = torch.nn.Linear(self.hidden_dim * self.num_heads, 1)
        self.semantic_attn2 = torch.nn.Linear(self.embed_dim, 1)
        self.norm1 = torch.nn.LayerNorm(self.hidden_dim * self.num_heads)
        self.norm2 = torch.nn.LayerNorm(self.embed_dim)
        self.relu = torch.nn.ReLU()

    def _semantic_attention(self, feat_dict: Dict[str, torch.Tensor], attn_layer: torch.nn.Linear) -> Dict[
        str, torch.Tensor]:
        etype_weights = {}
        for src_ntype, etype, dst_ntype in self.canonical_etypes:
            if src_ntype in feat_dict:
                avg_feat = torch.mean(feat_dict[src_ntype], dim=0, keepdim=True)
                weight = attn_layer(avg_feat).squeeze()
                etype_weights[etype] = weight

        weighted_feat = {}
        for ntype in feat_dict:
            related_weights = []
            for src_ntype, etype, dst_ntype in self.canonical_etypes:
                if src_ntype == ntype or dst_ntype == ntype:
                    related_weights.append(etype_weights[etype])
            if related_weights:
                weights = F.softmax(torch.stack(related_weights), dim=0)
                weighted_feat[ntype] = self.relu(feat_dict[ntype] * weights.mean())
            else:
                weighted_feat[ntype] = self.relu(feat_dict[ntype])
        return weighted_feat

    def forward(self) -> Dict[str, torch.Tensor]:
        x = {
            ntype: self.g.nodes[ntype].data["feat"]
            for ntype in self.node_types
        }

        # ç¬¬ä¸€å±‚è®¡ç®—
        h1 = self.hetero_conv1(self.g, x)
        h1 = {ntype: h.flatten(1) for ntype, h in h1.items()}
        h1 = self._semantic_attention(h1, self.semantic_attn1)
        h1 = {ntype: self.norm1(feat) for ntype, feat in h1.items()}

        # ç¬¬äºŒå±‚è®¡ç®—
        h2 = self.hetero_conv2(self.g, h1)
        h2 = {ntype: h.squeeze(1) for ntype, h in h2.items()}
        h2 = self._semantic_attention(h2, self.semantic_attn2)
        h2 = {ntype: self.norm2(feat) for ntype, feat in h2.items()}

        return h2


# ä¸‹è½½NLTKåœç”¨è¯èµ„æºï¼ˆé¦–æ¬¡è¿è¡Œéœ€å–æ¶ˆæ³¨é‡Šæ‰§è¡Œï¼‰
# nltk.download('stopwords')

# ===================== æ ¸å¿ƒé…ç½®å‚æ•° =====================
# æ•°æ®è·¯å¾„
SOURCE_CSV_PATH = "Data/parsed_whois.csv"  # æºWhoisæ•°æ®ï¼ˆå«inet6numã€netnameç­‰å­—æ®µï¼‰
MERGED_JSON_PATH = "Data/merged_whois.json"  # è¡¥å……Whoisæ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
GRAPH_PATH = "Data/ipv6_hetero_graph.bin"  # ç§å­å‰ç¼€å¼‚æ„å›¾ï¼ˆBINæ ¼å¼ï¼‰
ENTITY_MAP_PATH = "Data/entity_mapping.csv"  # å®ä½“IDæ˜ å°„è¡¨ï¼ˆç§å­æ•°æ®ï¼‰
MODEL_WEIGHT_PATH = "Data/han_model.pth"  # è®­ç»ƒå¥½çš„HANæ¨¡å‹æƒé‡
EMBEDDING_PATH = "Data/prefix_embeddings.npy"  # ç§å­å‰ç¼€åµŒå…¥ï¼ˆå¯é€‰ï¼Œç”¨äºå€™é€‰å®ä½“ç­›é€‰ï¼‰
OUTPUT_PATH = "completed_whois.csv"  # è¡¥å…¨ç»“æœè¾“å‡ºè·¯å¾„
UPDATED_GRAPH_PATH = "Data/ipv6_hetero_graph_updated.bin"  # æ›´æ–°åçš„å¼‚æ„å›¾ï¼ˆå«éç§å­å‰ç¼€ï¼‰

# æ¨¡å‹ä¸è¡¥å…¨å‚æ•°
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
EMBED_DIM = 64  # å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´
NUM_HEADS = 4  # å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´
CONFIDENCE_THRESHOLD = 0.7  # è®ºæ–‡è¦æ±‚çš„è¡¥å…¨ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆâ‰¥0.7æ‰å¡«å……ï¼‰
MIN_CANDIDATE_FREQ = 3  # å€™é€‰å®ä½“æœ€å°å‡ºç°é¢‘æ¬¡ï¼ˆè¿‡æ»¤ç¨€æœ‰å®ä½“ï¼Œæé«˜å‡†ç¡®æ€§ï¼‰
STOP_WORDS = set(stopwords.words('english'))  # è‹±æ–‡åœç”¨è¯ï¼ˆç”¨äºdescr_cleanedæ´¾ç”Ÿï¼‰

# ç‰¹å¾å·¥ç¨‹é…ç½®ï¼ˆä¸ç§å­å‰ç¼€ç‰¹å¾å®Œå…¨å¯¹é½ï¼Œç¡®ä¿33ç»´ï¼‰
PREFIX_FEAT_CONFIG = {
    "prefix_length": 1,  # å‰ç¼€é•¿åº¦ï¼ˆå¦‚/48â†’48ï¼‰
    "ipv6_version": 1,  # IPv6ç‰ˆæœ¬æ ‡è¯†ï¼ˆå›ºå®šä¸º1ï¼‰
    "net_type_onehot": 8,  # ç½‘ç»œç±»å‹ç‹¬çƒ­ç¼–ç ï¼ˆ8ç±»ï¼‰
    "country_onehot": 10,  # å›½å®¶ç‹¬çƒ­ç¼–ç ï¼ˆ10ç±»ï¼Œç§å­æ•°æ®ä¸­é«˜é¢‘å›½å®¶ï¼‰
    "keyword_bow": 10,  # å…³é”®å­—è¯è¢‹ç‰¹å¾ï¼ˆ10ç»´ï¼‰
    "status_onehot": 3  # çŠ¶æ€ç‹¬çƒ­ç¼–ç ï¼ˆ3ç±»ï¼‰
}
assert sum(PREFIX_FEAT_CONFIG.values()) == 33, "Prefixç‰¹å¾ç»´åº¦å¿…é¡»ä¸º33ç»´ï¼ˆä¸ç§å­å‰ç¼€ä¸€è‡´ï¼‰"


# ===================== è¡¥å…¨æ ¸å¿ƒç±» =====================
class WhoisCompleter:
    def __init__(self):
        # æ•°æ®å­˜å‚¨ç»“æ„
        self.source_df = None  # æºCSVæ•°æ®ï¼ˆå«ç§å­+éç§å­å‰ç¼€ï¼‰
        self.merged_df = None  # merged JSONæ•°æ®
        self.combined_df = None  # åˆå¹¶åçš„æ•°æ®
        self.seed_graph = None  # ç§å­å‰ç¼€å¼‚æ„å›¾
        self.temp_graph = None  # éç§å­å‰ç¼€ä¸´æ—¶å¼‚æ„å›¾
        self.updated_graph = None  # æ›´æ–°åçš„å®Œæ•´å¼‚æ„å›¾
        self.parent_mapping = {}  # å‰ç¼€-çˆ¶å‰ç¼€æ˜ å°„

        # å®ä½“æ˜ å°„è¡¨ï¼ˆåŒå‘æ˜ å°„ï¼šç§å­+éç§å­ï¼‰
        self.ent_val_to_id = defaultdict(dict)  # {å®ä½“ç±»å‹: {å®ä½“å€¼: åŸå§‹ID}}
        self.ent_id_to_val = defaultdict(dict)  # {å®ä½“ç±»å‹: {åŸå§‹ID: å®ä½“å€¼}}
        self.next_ent_ids = defaultdict(int)  # éç§å­å®ä½“çš„ä¸‹ä¸€ä¸ªå¯ç”¨åŸå§‹ID

        # æ–°å¢ï¼šè¿ç»­IDæ˜ å°„ï¼ˆè§£å†³DGLèŠ‚ç‚¹IDä¸è¿ç»­é—®é¢˜ï¼‰
        self.raw_to_continuous_id = defaultdict(dict)  # {å®ä½“ç±»å‹: {åŸå§‹ID: è¿ç»­ID}}
        self.continuous_to_raw_id = defaultdict(dict)  # {å®ä½“ç±»å‹: {è¿ç»­ID: åŸå§‹ID}}

        # æ¨¡å‹ç›¸å…³ï¼šä¿®å¤åˆå§‹åŒ–é—®é¢˜ï¼Œæ”¹ä¸ºåµŒå¥—defaultdict
        self.han_model = None  # åŠ è½½çš„è®­ç»ƒå¥½çš„HANæ¨¡å‹
        self.seed_embeddings = defaultdict(dict)  # {å®ä½“ç±»å‹: {å®ä½“å€¼: åµŒå…¥å‘é‡}} åµŒå¥—defaultdict
        self.non_seed_embeddings = {}  # éç§å­å®ä½“åµŒå…¥

        # å€™é€‰å®ä½“æ± ï¼ˆæŒ‰å®ä½“ç±»å‹åˆ†ç»„ï¼‰
        self.candidate_pool = defaultdict(list)  # {å®ä½“ç±»å‹: [(å®ä½“å€¼, å‡ºç°é¢‘æ¬¡)]}

    def load_entity_mapping(self) -> None:
        """åŠ è½½ç§å­å®ä½“æ˜ å°„è¡¨ï¼Œå¹¶åˆå§‹åŒ–éç§å­å®ä½“IDè®¡æ•°å™¨"""
        print("ğŸ” åŠ è½½å®ä½“æ˜ å°„è¡¨...")
        try:
            mapping_df = pd.read_csv(ENTITY_MAP_PATH)
            # æ„å»ºåŒå‘æ˜ å°„ï¼ˆç§å­å®ä½“ï¼‰
            for _, row in mapping_df.iterrows():
                ent_type = row["entity_type"]
                ent_val = row["entity_value"]
                ent_id = row["entity_id"]
                self.ent_val_to_id[ent_type][ent_val] = ent_id
                self.ent_id_to_val[ent_type][ent_id] = ent_val
            # åˆå§‹åŒ–éç§å­å®ä½“IDï¼ˆä»ç§å­æœ€å¤§ID+1å¼€å§‹ï¼‰
            for ent_type in self.ent_val_to_id:
                if self.ent_val_to_id[ent_type]:
                    self.next_ent_ids[ent_type] = max(self.ent_val_to_id[ent_type].values()) + 1
                else:
                    self.next_ent_ids[ent_type] = 0
            print(f"âœ… å®ä½“æ˜ å°„è¡¨åŠ è½½æˆåŠŸï¼šå…±{len(mapping_df)}ä¸ªç§å­å®ä½“ï¼Œæ¶µç›–{list(self.ent_val_to_id.keys())}ç±»å‹")
        except FileNotFoundError:
            raise FileNotFoundError(f"âŒ å®ä½“æ˜ å°„è¡¨ä¸å­˜åœ¨ï¼š{ENTITY_MAP_PATH}ï¼Œè¯·å…ˆè¿è¡ŒbuildGraph_6.pyç”Ÿæˆ")
        except Exception as e:
            raise RuntimeError(f"âŒ å®ä½“æ˜ å°„è¡¨åŠ è½½å¤±è´¥ï¼š{e}")

    def load_seed_embeddings(self) -> None:
        """åŠ è½½ç§å­å®ä½“åµŒå…¥ï¼ˆç”¨äºå€™é€‰å®ä½“ç­›é€‰å’Œç›¸ä¼¼åº¦è®¡ç®—ï¼‰"""
        print("ğŸ” åŠ è½½ç§å­å®ä½“åµŒå…¥...")
        try:
            import os
            # åŠ è½½PrefixåµŒå…¥ï¼ˆä¿®å¤KeyErrorï¼šå…ˆæ£€æŸ¥åµŒå…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•°æ®ï¼‰
            if os.path.exists(EMBEDDING_PATH):
                prefix_embed = np.load(EMBEDDING_PATH)
                # æ£€æŸ¥ç§å­Prefixæ•°é‡ä¸åµŒå…¥ç»´åº¦æ˜¯å¦åŒ¹é…
                seed_prefix_count = len(self.ent_val_to_id.get("Prefix", {}))
                if len(prefix_embed) >= seed_prefix_count and seed_prefix_count > 0:
                    for p_val, p_id in self.ent_val_to_id["Prefix"].items():
                        if p_id < len(prefix_embed):
                            self.seed_embeddings["Prefix"][p_val] = torch.tensor(prefix_embed[p_id], device=DEVICE)
                    print(f"   - PrefixåµŒå…¥åŠ è½½æˆåŠŸï¼š{len(self.seed_embeddings['Prefix'])}ä¸ªç§å­Prefix")
                else:
                    print(
                        f"âš ï¸ ç§å­PrefixåµŒå…¥ä¸å®Œæ•´ï¼šåµŒå…¥æ–‡ä»¶é•¿åº¦{len(prefix_embed)} < ç§å­Prefixæ•°é‡{seed_prefix_count}")
            else:
                print(f"âš ï¸ ç§å­PrefixåµŒå…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{EMBEDDING_PATH}")

            # åŠ è½½å…¶ä»–å®ä½“åµŒå…¥ï¼ˆä»æ¨¡å‹ä¸­æå–ï¼‰
            if self.han_model is not None:
                self.han_model.eval()
                with torch.no_grad():
                    seed_embeds = self.han_model()
                    for ent_type in ["Mnt", "Netname", "Country", "Status", "Keyword"]:
                        if ent_type in seed_embeds and ent_type in self.ent_val_to_id:
                            embed = seed_embeds[ent_type].detach().cpu().numpy()
                            ent_count = 0
                            for e_val, e_id in self.ent_val_to_id[ent_type].items():
                                if e_id < len(embed):
                                    self.seed_embeddings[ent_type][e_val] = torch.tensor(embed[e_id], device=DEVICE)
                                    ent_count += 1
                            print(f"   - {ent_type}åµŒå…¥åŠ è½½æˆåŠŸï¼š{ent_count}ä¸ªç§å­å®ä½“")
            print(f"âœ… ç§å­å®ä½“åµŒå…¥åŠ è½½å®Œæˆï¼šæ¶µç›–{[k for k, v in self.seed_embeddings.items() if v]}ç±»å‹")
        except Exception as e:
            print(f"âš ï¸ ç§å­å®ä½“åµŒå…¥åŠ è½½è­¦å‘Šï¼š{e}ï¼ˆå°†ä½¿ç”¨æ¨¡å‹å®æ—¶ç”Ÿæˆå€™é€‰å®ä½“åµŒå…¥ï¼‰")

    def load_han_model(self) -> None:
        """åŠ è½½è®­ç»ƒå¥½çš„HANæ¨¡å‹ï¼Œå›ºå®šæ‰€æœ‰å‚æ•°ï¼ˆä»…å‰å‘ä¼ æ’­ï¼‰"""
        print("ğŸ” åŠ è½½è®­ç»ƒå¥½çš„HANæ¨¡å‹...")
        try:
            # å…ˆåŠ è½½ç§å­å›¾è·å–è¾“å…¥ç‰¹å¾ç»´åº¦
            graphs, _ = dgl.load_graphs(GRAPH_PATH)
            self.seed_graph = graphs[0].to(DEVICE)
            # æ„å»ºè¾“å…¥ç‰¹å¾ç»´åº¦å­—å…¸
            in_feats_dict = {}
            for ntype in self.seed_graph.ntypes:
                in_feats_dict[ntype] = self.seed_graph.nodes[ntype].data["feat"].shape[1]
            # åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½æƒé‡
            self.han_model = HANModel(
                g=self.seed_graph,
                in_feats_dict=in_feats_dict,
                embed_dim=EMBED_DIM,
                num_heads=NUM_HEADS
            ).to(DEVICE)
            self.han_model.load_state_dict(torch.load(MODEL_WEIGHT_PATH, map_location=DEVICE))
            # å›ºå®šæ‰€æœ‰å‚æ•°ï¼ˆç¦æ­¢å¾®è°ƒï¼‰
            for param in self.han_model.parameters():
                param.requires_grad = False
            self.han_model.eval()
            print(f"âœ… HANæ¨¡å‹åŠ è½½æˆåŠŸï¼šæƒé‡è·¯å¾„={MODEL_WEIGHT_PATH}ï¼Œå‚æ•°å·²å›ºå®š")
        except FileNotFoundError:
            raise FileNotFoundError(f"âŒ æ¨¡å‹æƒé‡ä¸å­˜åœ¨ï¼š{MODEL_WEIGHT_PATH}ï¼Œè¯·å…ˆè¿è¡ŒHAN_train.pyè®­ç»ƒ")
        except Exception as e:
            raise RuntimeError(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")

    def load_data(self) -> None:
        """åŠ è½½æ‰€æœ‰æ•°æ®æºï¼ˆæºCSVã€merged JSONã€ç§å­å›¾ã€æ¨¡å‹ï¼‰"""
        print("\n===== åŠ è½½æ•°æ®æº =====")

        # 1. åŠ è½½æºCSVæ•°æ®ï¼ˆåŒºåˆ†ç§å­/éç§å­å‰ç¼€ï¼šç§å­å‰ç¼€åœ¨ç§å­å›¾ä¸­ï¼‰
        print("1. åŠ è½½æºWhoisæ•°æ®...")
        try:
            self.source_df = pd.read_csv(SOURCE_CSV_PATH)
            required_fields = ["inet6num", "original_inet6num"]
            missing_fields = [f for f in required_fields if f not in self.source_df.columns]
            if missing_fields:
                raise ValueError(f"æºæ•°æ®ç¼ºå¤±å…³é”®å­—æ®µï¼š{missing_fields}")
            # æ ‡è®°ç§å­/éç§å­å‰ç¼€ï¼ˆä¿®å¤ï¼šå¤„ç†ç§å­Prefixä¸ºç©ºçš„æƒ…å†µï¼‰
            seed_prefixes = set(self.ent_val_to_id.get("Prefix", {}).keys())
            self.source_df["is_seed"] = self.source_df["inet6num"].isin(seed_prefixes)
            seed_count = self.source_df["is_seed"].sum()
            non_seed_count = (~self.source_df["is_seed"]).sum()
            print(f"âœ… æºæ•°æ®åŠ è½½æˆåŠŸï¼šå…±{len(self.source_df)}æ¡è®°å½•ï¼ˆç§å­{seed_count}æ¡ï¼Œéç§å­{non_seed_count}æ¡ï¼‰")
            # è­¦å‘Šï¼šç§å­Prefixä¸ºç©ºå¯èƒ½å½±å“è¡¥å…¨æ•ˆæœ
            if seed_count == 0:
                print("âš ï¸ è­¦å‘Šï¼šæºæ•°æ®ä¸­æ— ç§å­å‰ç¼€ï¼ˆinet6numæœªåŒ¹é…åˆ°å®ä½“æ˜ å°„è¡¨ä¸­çš„Prefixï¼‰")
                print("   è¯·æ£€æŸ¥ï¼š1. entity_mapping.csvæ˜¯å¦åŒ…å«ç§å­Prefixï¼›2. parsed_whois.csvçš„inet6numæ ¼å¼æ˜¯å¦æ­£ç¡®")
        except FileNotFoundError:
            raise FileNotFoundError(f"âŒ æºæ•°æ®ä¸å­˜åœ¨ï¼š{SOURCE_CSV_PATH}")
        except Exception as e:
            raise RuntimeError(f"âŒ æºæ•°æ®åŠ è½½å¤±è´¥ï¼š{e}")

        # 2. åŠ è½½è¡¥å……Whoisæ•°æ®
        print("2. åŠ è½½è¡¥å……Whoisæ•°æ®...")
        try:
            with open(MERGED_JSON_PATH, "r", encoding="utf-8") as f:
                merged_dict = json.load(f)
            self.merged_df = pd.DataFrame.from_dict(merged_dict, orient="index").reset_index()
            self.merged_df.rename(columns={"index": "inet6num"}, inplace=True)
            print(f"âœ… è¡¥å……æ•°æ®åŠ è½½æˆåŠŸï¼š{len(self.merged_df)}æ¡è®°å½•")
        except FileNotFoundError:
            raise FileNotFoundError(f"âŒ è¡¥å……æ•°æ®ä¸å­˜åœ¨ï¼š{MERGED_JSON_PATH}")
        except Exception as e:
            raise RuntimeError(f"âŒ è¡¥å……æ•°æ®åŠ è½½å¤±è´¥ï¼š{e}")

        # 3. åŠ è½½å®ä½“æ˜ å°„è¡¨
        self.load_entity_mapping()

        # 4. åŠ è½½HANæ¨¡å‹
        self.load_han_model()

        # 5. åŠ è½½ç§å­å®ä½“åµŒå…¥
        self.load_seed_embeddings()

        # 6. æ„å»ºå‰ç¼€-çˆ¶å‰ç¼€æ˜ å°„
        self.parent_mapping = {
            row["inet6num"]: row["original_inet6num"]
            for _, row in self.source_df.iterrows()
            if pd.notna(row["original_inet6num"])
        }
        print(f"âœ… å‰ç¼€-çˆ¶å‰ç¼€æ˜ å°„æ„å»ºæˆåŠŸï¼š{len(self.parent_mapping)}æ¡æ˜ å°„å…³ç³»")

    def merge_data(self) -> None:
        """åˆå¹¶æºæ•°æ®ä¸è¡¥å……æ•°æ®ï¼ˆä»¥inet6numä¸ºå…³è”é”®ï¼‰"""
        print("\n===== åˆå¹¶æ•°æ®æº =====")
        self.combined_df = pd.merge(
            self.source_df,
            self.merged_df,
            on="inet6num",
            how="outer",
            suffixes=("", "_merged")
        )

        # åˆå§‹åŒ–å­—æ®µæ¥æºæ ‡è®°
        for col in ["netname", "descr", "country", "mnt-by", "status", "org", "descr_cleaned"]:
            if col in self.combined_df.columns:
                self.combined_df[f"{col}_source"] = "original"

        # æ ‡è®°éç§å­å‰ç¼€ï¼ˆåç»­ä»…å¯¹éç§å­æ‰§è¡Œæ¨¡å‹è¡¥å…¨ï¼‰
        seed_prefixes = set(self.ent_val_to_id.get("Prefix", {}).keys())
        self.combined_df["is_seed"] = self.combined_df["inet6num"].isin(seed_prefixes)
        non_seed_count = (~self.combined_df["is_seed"]).sum()
        print(f"âœ… æ•°æ®åˆå¹¶å®Œæˆï¼šå…±{len(self.combined_df)}æ¡è®°å½•ï¼ˆéç§å­{non_seed_count}æ¡ï¼‰")

    def basic_preprocessing(self) -> None:
        """åŸºç¡€é¢„å¤„ç†ï¼šæ´¾ç”Ÿdescr_cleaned + ç»Ÿä¸€å­—æ®µæ ¼å¼ï¼ˆä¸ç§å­å‰ç¼€ä¸€è‡´ï¼‰"""
        print("\n===== åŸºç¡€é¢„å¤„ç† =====")

        # 1. æ´¾ç”Ÿdescr_cleanedï¼ˆ33ç»´ç‰¹å¾çš„å…³é”®å­—æ¥æºï¼‰
        if "descr_cleaned" not in self.combined_df.columns:
            self.combined_df["descr_cleaned"] = np.nan
        mask = self.combined_df["descr_cleaned"].isna() & self.combined_df["descr"].notna()
        if mask.sum() > 0:
            def standardize_descr(descr: str) -> str:
                if not descr or str(descr).lower() == "nan":
                    return ""
                tokens = re.split(r"[\s-]+", str(descr).lower())
                filtered = [t for t in tokens if t.isalpha() and t not in STOP_WORDS and len(t) >= 3]
                return " ".join(filtered)

            self.combined_df.loc[mask, "descr_cleaned"] = self.combined_df.loc[mask, "descr"].apply(standardize_descr)
            self.combined_df.loc[mask, "descr_cleaned_source"] = "derived_from_descr"
        print(f"1. descr_cleanedæ´¾ç”Ÿï¼š{mask.sum()}æ¡")

        # 2. ç»Ÿä¸€å­—æ®µæ ¼å¼ï¼ˆå»é™¤ç©ºæ ¼ã€æ ‡å‡†åŒ–å¤§å°å†™ï¼‰
        str_fields = ["netname", "country", "mnt-by", "status", "org", "descr_cleaned"]
        for field in str_fields:
            if field in self.combined_df.columns:
                self.combined_df[field] = self.combined_df[field].astype(str).str.strip().str.upper()
                self.combined_df.loc[self.combined_df[field] == "NAN", field] = np.nan
        print(f"2. å­—æ®µæ ¼å¼æ ‡å‡†åŒ–å®Œæˆï¼š{str_fields}")

        # 3. åŸºç¡€è¡¥å…¨ï¼ˆå¤ç”¨è¡¥å……æ•°æ®ä¸­çš„éå†²çªå€¼ï¼‰
        mergeable_fields = [("descr", "descr_merged"), ("country", "country_merged"), ("org", "org_merged")]
        for target_col, merge_col in mergeable_fields:
            if target_col in self.combined_df.columns and merge_col in self.combined_df.columns:
                mask = self.combined_df[target_col].isna() & self.combined_df[merge_col].notna()
                if mask.sum() > 0:
                    self.combined_df.loc[mask, target_col] = self.combined_df.loc[mask, merge_col]
                    self.combined_df.loc[mask, f"{target_col}_source"] = "merged_data"
                print(f"3. {target_col}ä»è¡¥å……æ•°æ®è¡¥å…¨ï¼š{mask.sum()}æ¡")

    def generate_non_seed_features(self, prefix_val: str, row: pd.Series) -> torch.Tensor:
        """ç”Ÿæˆéç§å­Prefixçš„33ç»´ç‰¹å¾ï¼ˆä¸ç§å­å‰ç¼€å®Œå…¨å¯¹é½ï¼‰"""
        features = []

        # 1. å‰ç¼€é•¿åº¦ç‰¹å¾ï¼ˆ1ç»´ï¼‰ï¼šæå–/åçš„æ•°å­—ï¼ˆå¦‚2001::/48â†’48ï¼‰
        try:
            prefix_len = int(prefix_val.split("/")[-1])
            features.append(torch.tensor([prefix_len / 128.0], dtype=torch.float32))  # å½’ä¸€åŒ–åˆ°[0,1]
        except:
            features.append(torch.tensor([0.0], dtype=torch.float32))

        # 2. IPv6ç‰ˆæœ¬æ ‡è¯†ï¼ˆ1ç»´ï¼‰ï¼šå›ºå®šä¸º1
        features.append(torch.tensor([1.0], dtype=torch.float32))

        # 3. ç½‘ç»œç±»å‹ç‹¬çƒ­ç¼–ç ï¼ˆ8ç»´ï¼‰ï¼šåŸºäºå‰ç¼€é•¿åº¦åˆ†ç±»
        net_type = torch.zeros(8, dtype=torch.float32)
        try:
            prefix_len = int(prefix_val.split("/")[-1])
            if prefix_len <= 12:
                net_type[0] = 1.0  # éª¨å¹²ç½‘
            elif prefix_len <= 24:
                net_type[1] = 1.0  # åŒºåŸŸç½‘
            elif prefix_len <= 32:
                net_type[2] = 1.0  # éª¨å¹²æ¥å…¥ç½‘
            elif prefix_len <= 48:
                net_type[3] = 1.0  # æ ¡å›­ç½‘/ä¼ä¸šç½‘
            elif prefix_len <= 64:
                net_type[4] = 1.0  # å­ç½‘
            elif prefix_len <= 80:
                net_type[5] = 1.0  # ç»ˆç«¯ç½‘æ®µ
            elif prefix_len <= 96:
                net_type[6] = 1.0  # ç‰©è”ç½‘ç»ˆç«¯
            else:
                net_type[7] = 1.0  # å…¶ä»–
        except:
            net_type[7] = 1.0  # æœªçŸ¥ç±»å‹
        features.append(net_type)

        # 4. å›½å®¶ç‹¬çƒ­ç¼–ç ï¼ˆ10ç»´ï¼‰ï¼šç§å­æ•°æ®ä¸­é«˜é¢‘å›½å®¶ï¼ˆæŒ‰å‡ºç°é¢‘æ¬¡æ’åºï¼‰
        top_countries = ["US", "CN", "JP", "DE", "UK", "FR", "KR", "CA", "AU", "IN"]
        country_onehot = torch.zeros(10, dtype=torch.float32)
        if pd.notna(row["country"]) and row["country"] in top_countries:
            country_idx = top_countries.index(row["country"])
            country_onehot[country_idx] = 1.0
        features.append(country_onehot)

        # 5. å…³é”®å­—è¯è¢‹ç‰¹å¾ï¼ˆ10ç»´ï¼‰ï¼šåŸºäºdescr_cleanedçš„Top10é«˜é¢‘è¯
        top_keywords = ["INTERNET", "SERVICE", "PROVIDER", "NETWORK", "COMMUNICATION",
                        "TECHNOLOGY", "CORPORATION", "ORGANIZATION", "GOVERNMENT", "EDUCATION"]
        keyword_bow = torch.zeros(10, dtype=torch.float32)
        if pd.notna(row["descr_cleaned"]) and row["descr_cleaned"] != "":
            descr_tokens = row["descr_cleaned"].split()
            for i, kw in enumerate(top_keywords):
                if kw in descr_tokens:
                    keyword_bow[i] = 1.0
        features.append(keyword_bow)

        # 6. çŠ¶æ€ç‹¬çƒ­ç¼–ç ï¼ˆ3ç»´ï¼‰ï¼šALLOCATEDã€ASSIGNEDã€RESERVED
        status_onehot = torch.zeros(3, dtype=torch.float32)
        if pd.notna(row["status"]):
            status = row["status"].upper()
            if "ALLOCATED" in status:
                status_onehot[0] = 1.0
            elif "ASSIGNED" in status:
                status_onehot[1] = 1.0
            elif "RESERVED" in status:
                status_onehot[2] = 1.0
        features.append(status_onehot)

        # æ‹¼æ¥ä¸º33ç»´ç‰¹å¾
        return torch.cat(features, dim=0)

    def build_non_seed_graph(self) -> None:
        """æ„å»ºéç§å­å‰ç¼€çš„ä¸´æ—¶å¼‚æ„å›¾ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šè¿ç»­IDæ˜ å°„+ç‰¹å¾å…¨é‡å¡«å……ï¼‰"""
        print("\n===== æ„å»ºéç§å­å‰ç¼€ä¸´æ—¶å¼‚æ„å›¾ =====")
        non_seed_df = self.combined_df[~self.combined_df["is_seed"]].copy()
        if len(non_seed_df) == 0:
            print("âš ï¸ æ— æ˜¯éç§å­å‰ç¼€ï¼Œè·³è¿‡ä¸´æ—¶å›¾æ„å»º")
            return

        # ===================== æ­¥éª¤1ï¼šæ”¶é›†æ‰€æœ‰èŠ‚ç‚¹å’Œè¾¹ï¼ˆåŸå§‹IDï¼‰ =====================
        node_id_to_feat = defaultdict(dict)  # {å®ä½“ç±»å‹: {åŸå§‹ID: ç‰¹å¾å‘é‡}}
        edges = defaultdict(list)  # {è¾¹ç±»å‹ä¸‰å…ƒç»„: [(åŸå§‹æºID, åŸå§‹ç›®æ ‡ID)]}

        # å¤„ç†éç§å­PrefixèŠ‚ç‚¹
        non_seed_prefixes = []
        for idx, row in non_seed_df.iterrows():
            prefix_val = row["inet6num"]
            if pd.isna(prefix_val):
                continue
            # åˆ†é…åŸå§‹ID
            if prefix_val not in self.ent_val_to_id["Prefix"]:
                self.ent_val_to_id["Prefix"][prefix_val] = self.next_ent_ids["Prefix"]
                self.ent_id_to_val["Prefix"][self.next_ent_ids["Prefix"]] = prefix_val
                self.next_ent_ids["Prefix"] += 1
            prefix_id = self.ent_val_to_id["Prefix"][prefix_val]
            # ç”Ÿæˆç‰¹å¾ï¼ˆä»…å½“IDæœªå…³è”ç‰¹å¾æ—¶æ·»åŠ ï¼‰
            if prefix_id not in node_id_to_feat["Prefix"]:
                feat = self.generate_non_seed_features(prefix_val, row)
                node_id_to_feat["Prefix"][prefix_id] = feat
            non_seed_prefixes.append((prefix_val, prefix_id, row))

        # å¤„ç†å…³è”å®ä½“èŠ‚ç‚¹ï¼ˆMnt/Netname/Country/Status/Keywordï¼‰
        relation_mapping = {
            "mnt-by": ("Prefix", "prefix_to_mnt", "Mnt"),
            "netname": ("Prefix", "prefix_to_netname", "Netname"),
            "country": ("Prefix", "prefix_to_country", "Country"),
            "status": ("Prefix", "prefix_to_status", "Status"),
            "descr_cleaned": ("Prefix", "prefix_to_keyword", "Keyword")
        }
        for field, edge_type_triple in relation_mapping.items():
            src_type, etype_name, dst_type = edge_type_triple
            for prefix_val, prefix_id, row in non_seed_prefixes:
                ent_val = row[field]
                if pd.isna(ent_val) or ent_val == "" or str(ent_val).lower() == "nan":
                    continue
                # åˆ†é…åŸå§‹ID
                if ent_val not in self.ent_val_to_id[dst_type]:
                    # ç”Ÿæˆç‰¹å¾
                    if dst_type in self.seed_graph.ntypes:
                        feat_dim = self.seed_graph.nodes[dst_type].data["feat"].shape[1]
                    else:
                        feat_dim = 16
                    new_feat = torch.randn(feat_dim, dtype=torch.float32)
                    # åˆ†é…ID
                    ent_id = self.next_ent_ids[dst_type]
                    self.ent_val_to_id[dst_type][ent_val] = ent_id
                    self.ent_id_to_val[dst_type][ent_id] = ent_val
                    self.next_ent_ids[dst_type] += 1
                    node_id_to_feat[dst_type][ent_id] = new_feat
                else:
                    # ç§å­å®ä½“ï¼šä»ç§å­å›¾è·å–ç‰¹å¾
                    ent_id = self.ent_val_to_id[dst_type][ent_val]
                    if ent_id not in node_id_to_feat[dst_type]:
                        if dst_type in self.seed_graph.ntypes and ent_id < self.seed_graph.num_nodes(dst_type):
                            seed_feat = self.seed_graph.nodes[dst_type].data["feat"][ent_id]
                            node_id_to_feat[dst_type][ent_id] = seed_feat
                        else:
                            feat_dim = self.seed_graph.nodes[dst_type].data["feat"].shape[
                                1] if dst_type in self.seed_graph.ntypes else 16
                            node_id_to_feat[dst_type][ent_id] = torch.randn(feat_dim, dtype=torch.float32)
                # æ·»åŠ è¾¹ï¼ˆå»é‡ï¼‰
                if (prefix_id, ent_id) not in edges[edge_type_triple]:
                    edges[edge_type_triple].append((prefix_id, ent_id))

        # ===================== æ­¥éª¤2ï¼šç”Ÿæˆè¿ç»­IDæ˜ å°„ï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰ =====================
        self.raw_to_continuous_id.clear()
        self.continuous_to_raw_id.clear()
        max_raw_id = defaultdict(int)  # å„å®ä½“ç±»å‹çš„æœ€å¤§åŸå§‹ID

        # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—å„å®ä½“ç±»å‹çš„æœ€å¤§åŸå§‹IDï¼ˆç¡®å®šDGLçš„èŠ‚ç‚¹æ•°ï¼‰
        for ntype in node_id_to_feat:
            if node_id_to_feat[ntype]:
                max_raw_id[ntype] = max(node_id_to_feat[ntype].keys())
            else:
                max_raw_id[ntype] = 0

        # ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªå®ä½“ç±»å‹ç”Ÿæˆè¿ç»­IDæ˜ å°„ï¼ˆ0 â†’ max_raw_id[ntype]ï¼‰
        for ntype in node_id_to_feat:
            # åˆå§‹åŒ–è¿ç»­IDæ˜ å°„ï¼šè¦†ç›–æ‰€æœ‰0~max_raw_idçš„IDï¼ˆåŒ…æ‹¬ç©ºç¼ºï¼‰
            for raw_id in range(max_raw_id[ntype] + 1):
                self.raw_to_continuous_id[ntype][raw_id] = raw_id  # è¿ç»­ID = åŸå§‹IDï¼ˆç›´æ¥å¯¹é½ï¼‰
                self.continuous_to_raw_id[ntype][raw_id] = raw_id
            print(f"   - {ntype}ï¼šæœ€å¤§åŸå§‹ID={max_raw_id[ntype]} â†’ è¿ç»­IDèŒƒå›´=0~{max_raw_id[ntype]}")

        # ===================== æ­¥éª¤3ï¼šæ„å»ºå…¨é‡ç‰¹å¾çŸ©é˜µï¼ˆè¦†ç›–æ‰€æœ‰è¿ç»­IDï¼‰ =====================
        full_feat_matrix = {}
        for ntype in node_id_to_feat:
            max_cid = max_raw_id[ntype]
            # ç¡®å®šç‰¹å¾ç»´åº¦
            if ntype in self.seed_graph.ntypes:
                feat_dim = self.seed_graph.nodes[ntype].data["feat"].shape[1]
            else:
                feat_dim = 16 if ntype != "Prefix" else 33

            # åˆå§‹åŒ–å…¨é‡ç‰¹å¾çŸ©é˜µï¼ˆé»˜è®¤å€¼ï¼š0å‘é‡ï¼‰
            full_feat = torch.zeros((max_cid + 1, feat_dim), dtype=torch.float32)

            # å¡«å……å·²æœ‰èŠ‚ç‚¹çš„ç‰¹å¾
            for raw_id, feat in node_id_to_feat[ntype].items():
                cid = self.raw_to_continuous_id[ntype][raw_id]
                if cid <= max_cid:
                    full_feat[cid] = feat

            full_feat_matrix[ntype] = full_feat.to(DEVICE)
            print(
                f"   - {ntype}ï¼šå…¨é‡ç‰¹å¾çŸ©é˜µç»´åº¦={full_feat_matrix[ntype].shape}ï¼ˆèŠ‚ç‚¹æ•°={max_cid + 1}ï¼Œç‰¹å¾ç»´åº¦={feat_dim}ï¼‰")

        # ===================== æ­¥éª¤4ï¼šè½¬æ¢è¾¹ä¸ºè¿ç»­ID =====================
        graph_data = {}
        for edge_type_triple, edge_list in edges.items():
            if not edge_list:
                continue
            src_ntype, etype, dst_ntype = edge_type_triple
            # è½¬æ¢æº/ç›®æ ‡IDä¸ºè¿ç»­ID
            src_cids = []
            dst_cids = []
            for src_raw, dst_raw in edge_list:
                if src_raw in self.raw_to_continuous_id[src_ntype] and dst_raw in self.raw_to_continuous_id[dst_ntype]:
                    src_cids.append(self.raw_to_continuous_id[src_ntype][src_raw])
                    dst_cids.append(self.raw_to_continuous_id[dst_ntype][dst_raw])
            # æ·»åŠ åˆ°graph_data
            graph_data[edge_type_triple] = (
                torch.tensor(src_cids, dtype=torch.long),
                torch.tensor(dst_cids, dtype=torch.long)
            )

        # ===================== æ­¥éª¤5ï¼šæ„å»ºDGLå¼‚æ„å›¾å¹¶æ·»åŠ ç‰¹å¾ =====================
        self.temp_graph = dgl.heterograph(graph_data).to(DEVICE)

        # æ·»åŠ å…¨é‡ç‰¹å¾çŸ©é˜µï¼ˆç¡®ä¿ç‰¹å¾æ•°=èŠ‚ç‚¹æ•°ï¼‰
        for ntype in full_feat_matrix:
            # éªŒè¯ç‰¹å¾æ•°ä¸èŠ‚ç‚¹æ•°ä¸€è‡´
            assert full_feat_matrix[ntype].shape[0] == self.temp_graph.num_nodes(ntype), \
                f"âŒ {ntype}ç‰¹å¾æ•°({full_feat_matrix[ntype].shape[0]})ä¸èŠ‚ç‚¹æ•°({self.temp_graph.num_nodes(ntype)})ä¸åŒ¹é…"
            self.temp_graph.nodes[ntype].data["feat"] = full_feat_matrix[ntype]

        # æœ€ç»ˆæ ¡éªŒ
        print(f"âœ… éç§å­ä¸´æ—¶å¼‚æ„å›¾æ„å»ºå®Œæˆï¼š")
        print(f"   - èŠ‚ç‚¹ç±»å‹ï¼š{list(self.temp_graph.ntypes)}")
        print(f"   - è¾¹ç±»å‹ï¼š{[rel[1] for rel in self.temp_graph.canonical_etypes]}")
        print(
            f"   - PrefixèŠ‚ç‚¹æ•°ï¼š{self.temp_graph.num_nodes('Prefix')}ï¼Œç‰¹å¾æ•°ï¼š{self.temp_graph.nodes['Prefix'].data['feat'].shape[0]}")

    def build_candidate_pool(self) -> None:
        """æ„å»ºå€™é€‰å®ä½“æ± ï¼ˆç§å­æ•°æ®ä¸­é«˜é¢‘å®ä½“ï¼Œè¿‡æ»¤ç¨€æœ‰å®ä½“ï¼‰"""
        print("\n===== æ„å»ºå€™é€‰å®ä½“æ±  =====")
        # å€™é€‰å®ä½“ç±»å‹ä¸å­—æ®µæ˜ å°„
        candidate_types = {
            "Mnt": "mnt-by",
            "Netname": "netname",
            "Country": "country",
            "Status": "status"
        }
        # ç»Ÿè®¡ç§å­æ•°æ®ä¸­å„å®ä½“çš„å‡ºç°é¢‘æ¬¡
        for ent_type, field in candidate_types.items():
            edge_type_triple = ("Prefix", f"prefix_to_{ent_type.lower()}", ent_type)
            # æ£€æŸ¥è¾¹ç±»å‹æ˜¯å¦å­˜åœ¨
            if edge_type_triple in self.seed_graph.canonical_etypes:
                src_ids, dst_ids = self.seed_graph.edges(etype=edge_type_triple)
                ent_counts = Counter(dst_ids.cpu().numpy())
                # ç­›é€‰é«˜é¢‘å®ä½“ï¼ˆâ‰¥MIN_CANDIDATE_FREQï¼‰
                for ent_id, count in ent_counts.items():
                    if count >= MIN_CANDIDATE_FREQ:
                        ent_val = self.ent_id_to_val[ent_type].get(ent_id)
                        if ent_val:
                            self.candidate_pool[ent_type].append((ent_val, count))
            # æŒ‰é¢‘æ¬¡æ’åº
            self.candidate_pool[ent_type].sort(key=lambda x: x[1], reverse=True)
            print(f"   - {ent_type}ï¼š{len(self.candidate_pool[ent_type])}ä¸ªå€™é€‰å®ä½“ï¼ˆé¢‘æ¬¡â‰¥{MIN_CANDIDATE_FREQ}ï¼‰")

    def calculate_association_prob(self, prefix_embed: torch.Tensor, candidate_embeds: List[torch.Tensor]) -> List[
        float]:
        """æŒ‰è®ºæ–‡å…¬å¼è®¡ç®—éç§å­Prefixä¸å€™é€‰å®ä½“çš„å…³è”æ¦‚ç‡ï¼ˆç‚¹ç§¯+sigmoidï¼‰"""
        # L2å½’ä¸€åŒ–ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        prefix_embed = F.normalize(prefix_embed, p=2, dim=0)
        candidate_embeds = [F.normalize(embed, p=2, dim=0) for embed in candidate_embeds]
        # è®¡ç®—ç‚¹ç§¯å¹¶æ˜ å°„åˆ°[0,1]
        probs = []
        for cand_embed in candidate_embeds:
            dot_product = torch.sum(prefix_embed * cand_embed)
            prob = torch.sigmoid(dot_product).item()
            probs.append(prob)
        return probs

    def check_consistency(self, prefix_row: pd.Series, completed_ent_type: str, completed_val: str) -> bool:
        """ä¸€è‡´æ€§æ ¡éªŒï¼šè¡¥å…¨ç»“æœä¸å·²æœ‰ä¿¡æ¯æ— å†²çªï¼ˆè®ºæ–‡è¦æ±‚ï¼‰"""
        # 1. Mntä¸Countryå†²çªæ ¡éªŒï¼ˆMntåç§°å«å›½å®¶ç ï¼Œéœ€ä¸Prefixçš„Countryä¸€è‡´ï¼‰
        if completed_ent_type == "Mnt" and pd.notna(prefix_row["country"]):
            # æå–Mntä¸­çš„å›½å®¶ç ï¼ˆå¦‚MAINT-JP-WIDEâ†’JPï¼‰
            country_code = prefix_row["country"].upper()
            if country_code in completed_val:
                return True
            else:
                # å¸¸è§å›½å®¶ç æ˜ å°„ï¼ˆå¤„ç†ç¼©å†™å˜ä½“ï¼‰
                country_map = {"US": ["USA", "AMERICA"], "CN": ["CHINA", "PRC"], "JP": ["JAPAN"],
                               "DE": ["GERMANY"], "UK": ["BRITAIN", "UNITEDKINGDOM"]}
                for code, variants in country_map.items():
                    if country_code == code and any(var in completed_val for var in variants):
                        return True
                print(f"âš ï¸ å†²çªï¼šMnt={completed_val} ä¸ Country={country_code} ä¸åŒ¹é…ï¼Œä¸¢å¼ƒè¡¥å…¨ç»“æœ")
                return False
        # 2. å…¶ä»–å®ä½“ç±»å‹æš‚æ— éœ€æ ¡éªŒï¼ˆå¯æ‰©å±•ï¼‰
        return True

    def model_based_completion(self) -> None:
        """æ¨¡å‹åŸºè¡¥å…¨ï¼šåˆ©ç”¨è®­ç»ƒå¥½çš„HANæ¨¡å‹é¢„æµ‹å…³è”æ¦‚ç‡ï¼ˆè®ºæ–‡æ ¸å¿ƒé€»è¾‘ï¼‰"""
        print("\n===== æ¨¡å‹åŸºè¡¥å…¨ï¼ˆè®ºæ–‡3.4.1èŠ‚æ ¸å¿ƒï¼‰ =====")
        non_seed_df = self.combined_df[~self.combined_df["is_seed"]].copy()
        if len(non_seed_df) == 0 or self.temp_graph is None:
            print("âš ï¸ æ— æ˜¯éç§å­å‰ç¼€æˆ–ä¸´æ—¶å›¾æœªæ„å»ºï¼Œè·³è¿‡æ¨¡å‹åŸºè¡¥å…¨")
            return

        # 1. ç”Ÿæˆéç§å­å®ä½“åµŒå…¥ï¼ˆå›ºå®šæ¨¡å‹å‚æ•°ï¼Œä»…å‰å‘ä¼ æ’­ï¼‰
        print("1. ç”Ÿæˆéç§å­å®ä½“åµŒå…¥ï¼ˆL2å½’ä¸€åŒ–ï¼‰...")
        self.han_model.g = self.temp_graph  # æ›¿æ¢ä¸ºéç§å­ä¸´æ—¶å›¾
        with torch.no_grad():
            non_seed_embeds = self.han_model()
        # æå–éç§å­PrefixåµŒå…¥å¹¶L2å½’ä¸€åŒ–
        if "Prefix" not in non_seed_embeds:
            print("âš ï¸ æœªç”Ÿæˆéç§å­PrefixåµŒå…¥ï¼Œè·³è¿‡æ¨¡å‹åŸºè¡¥å…¨")
            return
        prefix_embeds = non_seed_embeds["Prefix"]
        prefix_embeds = F.normalize(prefix_embeds, p=2, dim=1)
        print(f"   - éç§å­PrefixåµŒå…¥ç”Ÿæˆå®Œæˆï¼š{prefix_embeds.shape[0]}ä¸ªå‘é‡ï¼ˆç»´åº¦{prefix_embeds.shape[1]}ï¼‰")

        # 2. è¡¥å…¨è§„åˆ™ï¼ˆå®ä½“ç±»å‹â†’å­—æ®µâ†’è¾¹ç±»å‹ä¸‰å…ƒç»„ï¼‰
        completion_rules = [
            {
                "ent_type": "Mnt",
                "target_col": "mnt-by",
                "edge_type_triple": ("Prefix", "prefix_to_mnt", "Mnt")
            },
            {
                "ent_type": "Netname",
                "target_col": "netname",
                "edge_type_triple": ("Prefix", "prefix_to_netname", "Netname")
            },
            {
                "ent_type": "Country",
                "target_col": "country",
                "edge_type_triple": ("Prefix", "prefix_to_country", "Country")
            },
            {
                "ent_type": "Status",
                "target_col": "status",
                "edge_type_triple": ("Prefix", "prefix_to_status", "Status")
            }
        ]

        # 3. å¯¹æ¯ä¸ªå­—æ®µæ‰§è¡Œè¡¥å…¨
        for rule in completion_rules:
            ent_type = rule["ent_type"]
            target_col = rule["target_col"]
            edge_type_triple = rule["edge_type_triple"]

            # è·³è¿‡ï¼šå€™é€‰æ± ä¸ºç©ºæˆ–å­—æ®µä¸å­˜åœ¨
            if ent_type not in self.candidate_pool or len(self.candidate_pool[ent_type]) == 0:
                print(f"âš ï¸ è·³è¿‡{target_col}ï¼šæ— å€™é€‰å®ä½“")
                continue
            if target_col not in self.combined_df.columns:
                print(f"âš ï¸ è·³è¿‡{target_col}ï¼šæ•°æ®ä¸­æ— æ­¤å­—æ®µ")
                continue

            # å‡†å¤‡å€™é€‰å®ä½“åµŒå…¥ï¼ˆå…¼å®¹ç§å­åµŒå…¥ç¼ºå¤±çš„æƒ…å†µï¼‰
            candidate_vals = [c[0] for c in self.candidate_pool[ent_type]]
            candidate_embeds = []
            for val in candidate_vals:
                if val in self.seed_embeddings[ent_type]:
                    candidate_embeds.append(self.seed_embeddings[ent_type][val])
                else:
                    # ç§å­åµŒå…¥ç¼ºå¤±æ—¶ï¼Œä»ç§å­å›¾å®æ—¶ç”Ÿæˆ
                    if ent_type in self.seed_graph.ntypes and val in self.ent_val_to_id[ent_type]:
                        ent_id = self.ent_val_to_id[ent_type][val]
                        with torch.no_grad():
                            seed_embeds = self.han_model()
                            if ent_type in seed_embeds and ent_id < len(seed_embeds[ent_type]):
                                cand_embed = seed_embeds[ent_type][ent_id]
                                candidate_embeds.append(cand_embed)
            if len(candidate_embeds) == 0:
                print(f"âš ï¸ è·³è¿‡{target_col}ï¼šæ— å€™é€‰å®ä½“åµŒå…¥")
                continue

            # ç­›é€‰å¾…è¡¥å…¨çš„éç§å­å‰ç¼€
            mask = (self.combined_df["is_seed"] == False) & (self.combined_df[target_col].isna())
            count = 0

            for idx, row in self.combined_df[mask].iterrows():
                prefix_val = row["inet6num"]
                # è·å–éç§å­Prefixçš„åŸå§‹IDå’Œè¿ç»­ID
                prefix_raw_id = self.ent_val_to_id["Prefix"].get(prefix_val)
                if prefix_raw_id is None:
                    continue
                # è½¬æ¢ä¸ºè¿ç»­IDï¼ˆåµŒå…¥ç´¢å¼•ï¼‰
                if prefix_raw_id not in self.raw_to_continuous_id["Prefix"]:
                    continue
                prefix_cid = self.raw_to_continuous_id["Prefix"][prefix_raw_id]
                if prefix_cid >= len(prefix_embeds):
                    continue
                prefix_embed = prefix_embeds[prefix_cid]

                # è®¡ç®—ä¸æ‰€æœ‰å€™é€‰å®ä½“çš„å…³è”æ¦‚ç‡
                probs = self.calculate_association_prob(prefix_embed, candidate_embeds)
                max_prob_idx = np.argmax(probs)
                max_prob = probs[max_prob_idx]
                best_candidate = candidate_vals[max_prob_idx]

                # æ»¡è¶³ç½®ä¿¡åº¦é˜ˆå€¼ä¸”ä¸€è‡´æ€§æ ¡éªŒé€šè¿‡
                if max_prob >= CONFIDENCE_THRESHOLD:
                    if self.check_consistency(row, ent_type, best_candidate):
                        # è¡¥å…¨å­—æ®µå¹¶æ ‡è®°æ¥æº
                        self.combined_df.at[idx, target_col] = best_candidate
                        self.combined_df.at[idx, f"{target_col}_source"] = f"model_pred(prob={max_prob:.3f})"
                        count += 1
                        # å‘ä¸´æ—¶å›¾æ·»åŠ è¡¥å…¨çš„è¾¹ï¼ˆä½¿ç”¨è¿ç»­IDï¼‰
                        ent_raw_id = self.ent_val_to_id[ent_type][best_candidate]
                        ent_cid = self.raw_to_continuous_id[ent_type][ent_raw_id]
                        if edge_type_triple not in self.temp_graph.canonical_etypes:
                            self.temp_graph.add_edges([prefix_cid], [ent_cid], etype=edge_type_triple)
                        else:
                            self.temp_graph.add_edges([prefix_cid], [ent_cid], etype=edge_type_triple)

            print(f"   - {target_col}ï¼š{count}æ¡è¡¥å…¨ï¼ˆç½®ä¿¡åº¦â‰¥{CONFIDENCE_THRESHOLD}ï¼‰")

    def update_hetero_graph(self) -> None:
        """æ›´æ–°å¼‚æ„å›¾ï¼šåˆå¹¶ç§å­å›¾ä¸éç§å­å›¾ï¼ˆå«è¡¥å…¨çš„è¾¹ï¼‰"""
        print("\n===== æ›´æ–°å¼‚æ„å›¾ =====")
        if self.seed_graph is None or self.temp_graph is None:
            print("âš ï¸ ç§å­å›¾æˆ–ä¸´æ—¶å›¾æœªæ„å»ºï¼Œè·³è¿‡å¼‚æ„å›¾æ›´æ–°")
            return

        # 1. åˆå¹¶èŠ‚ç‚¹ï¼ˆç§å­èŠ‚ç‚¹+éç§å­èŠ‚ç‚¹ï¼‰
        merged_feats = {}
        for ntype in self.seed_graph.ntypes:
            # ç§å­èŠ‚ç‚¹ç‰¹å¾
            seed_feats = self.seed_graph.nodes[ntype].data["feat"]
            # éç§å­èŠ‚ç‚¹ç‰¹å¾ï¼ˆè‹¥å­˜åœ¨ï¼‰
            if ntype in self.temp_graph.ntypes and "feat" in self.temp_graph.nodes[ntype].data:
                non_seed_feats = self.temp_graph.nodes[ntype].data["feat"]
                # åˆå¹¶ç‰¹å¾ï¼ˆæŒ‰IDé¡ºåºï¼Œéç§å­IDåœ¨ç§å­ä¹‹åï¼‰
                merged_feats[ntype] = torch.cat([seed_feats, non_seed_feats], dim=0)
            else:
                merged_feats[ntype] = seed_feats

        # 2. åˆå¹¶è¾¹ï¼ˆç§å­è¾¹+éç§å­è¾¹+è¡¥å…¨è¾¹ï¼‰
        merged_edges = {}
        for edge_type_triple in self.seed_graph.canonical_etypes:
            # ç§å­è¾¹
            seed_src, seed_dst = self.seed_graph.edges(etype=edge_type_triple)
            # éç§å­è¾¹ï¼ˆè‹¥å­˜åœ¨ï¼‰
            if edge_type_triple in self.temp_graph.canonical_etypes:
                non_seed_src, non_seed_dst = self.temp_graph.edges(etype=edge_type_triple)
                # éç§å­è¾¹IDåç§»ï¼ˆé¿å…ä¸ç§å­IDå†²çªï¼‰
                seed_node_count = self.seed_graph.num_nodes(edge_type_triple[0])
                non_seed_src_shifted = non_seed_src + seed_node_count
                non_seed_dst_shifted = non_seed_dst + self.seed_graph.num_nodes(edge_type_triple[2])
                # åˆå¹¶
                merged_src = torch.cat([seed_src, non_seed_src_shifted], dim=0)
                merged_dst = torch.cat([seed_dst, non_seed_dst_shifted], dim=0)
                merged_edges[edge_type_triple] = (merged_src, merged_dst)
            else:
                merged_edges[edge_type_triple] = (seed_src, seed_dst)

        # 3. æ„å»ºæ›´æ–°åçš„å¼‚æ„å›¾
        self.updated_graph = dgl.heterograph(merged_edges).to(DEVICE)
        for ntype in merged_feats.keys():
            self.updated_graph.nodes[ntype].data["feat"] = merged_feats[ntype]

        # 4. ä¿å­˜æ›´æ–°åçš„å›¾
        dgl.save_graphs(UPDATED_GRAPH_PATH, [self.updated_graph])
        print(f"âœ… å¼‚æ„å›¾æ›´æ–°å®Œæˆå¹¶ä¿å­˜è‡³ï¼š{UPDATED_GRAPH_PATH}")
        print(f"   - æ€»èŠ‚ç‚¹æ•°ï¼š{sum(self.updated_graph.num_nodes(ntype) for ntype in self.updated_graph.ntypes)}")
        print(
            f"   - æ€»è¾¹æ•°ï¼š{sum(self.updated_graph.num_edges(etype) for etype in self.updated_graph.canonical_etypes)}")

    def save_results(self) -> None:
        """ä¿å­˜è¡¥å…¨ç»“æœå¹¶è¾“å‡ºç»Ÿè®¡æŠ¥å‘Š"""
        print("\n===== è¡¥å…¨ç»“æœç»Ÿè®¡ä¸ä¿å­˜ =====")
        # è¡¥å…¨æ•ˆæœç»Ÿè®¡
        completion_stats = []
        for col in ["netname", "descr", "country", "mnt-by", "status", "org", "descr_cleaned"]:
            if col in self.combined_df.columns:
                total = len(self.combined_df)
                missing = self.combined_df[col].isna().sum()
                completed = total - missing
                completion_rate = (completed / total) * 100 if total > 0 else 0
                # æŒ‰ç§å­/éç§å­æ‹†åˆ†ç»Ÿè®¡
                seed_completed = self.combined_df[self.combined_df["is_seed"]][col].notna().sum()
                seed_total = self.combined_df["is_seed"].sum()
                non_seed_completed = completed - seed_completed
                non_seed_total = total - seed_total
                non_seed_completion_rate = (non_seed_completed / non_seed_total) * 100 if non_seed_total > 0 else 0
                completion_stats.append({
                    "å­—æ®µ": col,
                    "æ€»è®°å½•æ•°": total,
                    "æ€»è¡¥å…¨ç‡": f"{completion_rate:.1f}%",
                    "éç§å­è®°å½•æ•°": non_seed_total,
                    "éç§å­è¡¥å…¨ç‡": f"{non_seed_completion_rate:.1f}%"
                })

        # æ‰“å°ç»Ÿè®¡è¡¨æ ¼
        stats_df = pd.DataFrame(completion_stats)
        print(stats_df.to_string(index=False))

        # ä¿å­˜è¡¥å…¨ç»“æœ
        try:
            self.combined_df.to_csv(OUTPUT_PATH, index=False, encoding="utf-8")
            print(f"\nâœ… è¡¥å…¨ç»“æœå·²ä¿å­˜è‡³ï¼š{OUTPUT_PATH}")
        except Exception as e:
            raise RuntimeError(f"âŒ ç»“æœä¿å­˜å¤±è´¥ï¼š{e}")

    def run(self) -> None:
        """æ‰§è¡Œå®Œæ•´è¡¥å…¨æµç¨‹ï¼ˆä¸¥æ ¼éµå¾ªè®ºæ–‡3.4.1èŠ‚æ­¥éª¤ï¼‰"""
        print("=" * 60)
        print("ğŸ“š éç§å­å‰ç¼€Whoisä¿¡æ¯è¡¥å…¨æµç¨‹ï¼ˆè®ºæ–‡3.4.1èŠ‚ï¼‰")
        print("=" * 60)

        try:
            # è®ºæ–‡è¦æ±‚çš„æµç¨‹é¡ºåºï¼š
            # 1. æ•°æ®åŠ è½½ä¸åˆå¹¶
            self.load_data()
            self.merge_data()
            # 2. éç§å­å‰ç¼€é¢„å¤„ç†ï¼ˆç”Ÿæˆ33ç»´ç‰¹å¾ï¼‰
            self.basic_preprocessing()
            # 3. æ„å»ºéç§å­ä¸´æ—¶å¼‚æ„å›¾
            self.build_non_seed_graph()
            # 4. æ„å»ºå€™é€‰å®ä½“æ± 
            self.build_candidate_pool()
            # 5. æ¨¡å‹åŸºè¡¥å…¨ï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼‰
            self.model_based_completion()
            # 6. æ›´æ–°å¼‚æ„å›¾ï¼ˆå«è¡¥å…¨è¾¹ï¼‰
            self.update_hetero_graph()
            # 7. ä¿å­˜ç»“æœ
            self.save_results()

            print("\n" + "=" * 60)
            print("âœ… è¡¥å…¨æµç¨‹å…¨éƒ¨å®Œæˆï¼")
            print("=" * 60)
        except Exception as e:
            # å¼‚å¸¸æ—¶ä¿å­˜éƒ¨åˆ†ç»“æœ
            if self.combined_df is not None:
                partial_path = f"partial_{OUTPUT_PATH}"
                self.combined_df.to_csv(partial_path, index=False, encoding="utf-8")
                print(f"\nâš ï¸ è¡¥å…¨æµç¨‹å¼‚å¸¸ä¸­æ–­ï¼Œå·²ä¿å­˜éƒ¨åˆ†ç»“æœè‡³ï¼š{partial_path}")
            raise RuntimeError(f"âŒ è¡¥å…¨æµç¨‹å¤±è´¥ï¼š{e}")


# ===================== æ‰§è¡Œå…¥å£ =====================
if __name__ == "__main__":
    import os  # æ–°å¢oså¯¼å…¥ï¼Œç”¨äºæ–‡ä»¶å­˜åœ¨æ€§åˆ¤æ–­

    try:
        completer = WhoisCompleter()
        completer.run()
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼š{e}")
        import traceback

        traceback.print_exc()
        exit(1)