import pandas as pd
import json
import re
import dgl
import numpy as np
import torch
import torch.nn.functional as F
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Union, Optional
import nltk
from nltk.corpus import stopwords
from tqdm import tqdm

# å…¨å±€é…ç½®
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
EMBED_DIM = 64  # è®ºæ–‡æŒ‡å®š64ç»´åµŒå…¥
CONFIDENCE_THRESHOLD = 0.7  # è®ºæ–‡è¦æ±‚é˜ˆå€¼0.7
MIN_CANDIDATE_FREQ = 3  # å€™é€‰å®ä½“æœ€å°é¢‘æ¬¡
STOP_WORDS = set(stopwords.words('english'))

# ===================== æ ¸å¿ƒå‚æ•°ï¼ˆå¿…é¡»å’Œè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰ =====================
TRAIN_HIDDEN_DIM = 16
TRAIN_NUM_HEADS = 4


# ===================== æ¨¡å‹ç»“æ„ =====================
class FeatureMappingLayer(torch.nn.Module):
    """ç‰¹å¾æ˜ å°„å±‚ï¼ˆæ ¸å¿ƒå±‚ï¼šåŒ¹é…è®­ç»ƒæƒé‡ï¼‰"""

    def __init__(self, in_feats_dict: Dict[str, int], out_dim: int = 64):
        super().__init__()
        self.linear_layers = torch.nn.ModuleDict()
        for ntype, in_dim in in_feats_dict.items():
            self.linear_layers[ntype] = torch.nn.Linear(in_dim, out_dim)

    def forward(self, g: dgl.DGLHeteroGraph) -> Dict[str, torch.Tensor]:
        h = {}
        for ntype in g.ntypes:
            if ntype in self.linear_layers and 'feat' in g.nodes[ntype].data:
                h[ntype] = F.relu(self.linear_layers[ntype](g.nodes[ntype].data['feat']))
            else:
                h[ntype] = torch.zeros((g.num_nodes(ntype), 64), device=DEVICE)
        return h


class NodeAttentionLayer(torch.nn.Module):
    """èŠ‚ç‚¹æ³¨æ„åŠ›å±‚ï¼ˆé€‚é…å¼‚æ„å›¾GATConvï¼‰"""

    def __init__(self, edge_types: List[str], in_dim: int = 64, hidden_dim: int = 16, num_heads: int = 4):
        super().__init__()
        self.num_heads = num_heads
        self.hidden_dim = hidden_dim

        # æ ¸å¿ƒå±‚ï¼šåŒ¹é…è®­ç»ƒæƒé‡
        self.W = torch.nn.Linear(in_dim, in_dim)
        self.a = torch.nn.Linear(2 * in_dim, 1)

        # GATå±‚ï¼šåˆå§‹åŒ–
        self.gat_layers = torch.nn.ModuleDict()
        for etype in edge_types:
            self.gat_layers[etype] = dgl.nn.GATConv(
                in_feats=in_dim,
                out_feats=hidden_dim,
                num_heads=num_heads,
                allow_zero_in_degree=True
            )

    def forward(self, g: dgl.DGLHeteroGraph, h: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """å¼‚æ„å›¾GATConvå‰å‘"""
        gat_out = {}

        # éå†æ‰€æœ‰è¾¹ç±»å‹å¤„ç†GAT
        for stype, etype, dtype in g.canonical_etypes:
            if etype not in self.gat_layers:
                continue
            if stype not in h or dtype not in h:
                continue

            try:
                # å¼‚æ„å›¾GATConvè¾“å…¥ï¼šå­—å…¸æ ¼å¼
                feat_src = {stype: h[stype]}
                feat_dst = {dtype: h[dtype]}
                out = self.gat_layers[etype](g[stype, etype, dtype], (feat_src, feat_dst))
                out_tensor = out[dtype].flatten(1)  # [N, num_heads*hidden_dim]
            except:
                # å…¼å®¹å•èŠ‚ç‚¹ç±»å‹è¾“å…¥
                out = self.gat_layers[etype](g[stype, etype, dtype], (h[stype], h[dtype]))
                out_tensor = out.flatten(1)

            # èšåˆåˆ°ç›®æ ‡èŠ‚ç‚¹ç±»å‹
            if dtype not in gat_out:
                gat_out[dtype] = []
            gat_out[dtype].append(out_tensor)

        # èšåˆæ‰€æœ‰è¾¹ç±»å‹çš„è¾“å‡º
        h_out = {}
        for ntype in g.ntypes:
            if ntype in gat_out and len(gat_out[ntype]) > 0:
                h_out[ntype] = torch.stack(gat_out[ntype], dim=0).sum(dim=0)
            else:
                h_out[ntype] = torch.zeros((g.num_nodes(ntype), self.num_heads * self.hidden_dim), device=DEVICE)

        return h_out


class HANLinkPredModel(torch.nn.Module):
    """HANæ¨¡å‹ï¼ˆé€‚é…å¼‚æ„å›¾ï¼‰"""

    def __init__(self,
                 g: dgl.DGLHeteroGraph,
                 in_feats_dict: Dict[str, int]):
        super().__init__()
        self.g = g
        self.in_feats_dict = in_feats_dict

        # 1. æ ¸å¿ƒç‰¹å¾æ˜ å°„å±‚
        self.feature_mapping = FeatureMappingLayer(in_feats_dict, out_dim=64)

        # 2. æ ¸å¿ƒèŠ‚ç‚¹æ³¨æ„åŠ›å±‚
        edge_types = [e[1] for e in g.canonical_etypes]
        self.node_attention = NodeAttentionLayer(
            edge_types=edge_types,
            in_dim=64,
            hidden_dim=TRAIN_HIDDEN_DIM,
            num_heads=TRAIN_NUM_HEADS
        )

        # 3. ç¬¬äºŒå±‚GATï¼ˆé€‚é…å¼‚æ„å›¾ï¼‰
        self.gat2 = torch.nn.ModuleDict()
        for stype, etype, dtype in g.canonical_etypes:
            self.gat2[etype] = dgl.nn.GATConv(
                in_feats=64,
                out_feats=64,
                num_heads=1,
                allow_zero_in_degree=True
            )

    def forward(self, input_g: Optional[dgl.DGLHeteroGraph] = None) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­ï¼šä»…ç”ŸæˆåµŒå…¥"""
        g = input_g if input_g is not None else self.g

        # 1. ç‰¹å¾æ˜ å°„ï¼ˆè¾“å‡ºï¼š{èŠ‚ç‚¹ç±»å‹: 64ç»´ç‰¹å¾}ï¼‰
        h = self.feature_mapping(g)

        # 2. èŠ‚ç‚¹æ³¨æ„åŠ›ï¼ˆè¾“å‡ºï¼š{èŠ‚ç‚¹ç±»å‹: 64ç»´ç‰¹å¾}ï¼‰
        h1 = self.node_attention(g, h)

        # 3. ç¬¬äºŒå±‚GAT
        h2 = {}
        for stype, etype, dtype in g.canonical_etypes:
            if etype not in self.gat2:
                continue
            if stype not in h1 or dtype not in h1:
                continue

            try:
                # å¼‚æ„å›¾è¾“å…¥æ ¼å¼
                feat_src = {stype: h1[stype][:, :64]}  # ç¡®ä¿64ç»´è¾“å…¥
                feat_dst = {dtype: h1[dtype][:, :64]}
                out = self.gat2[etype](g[stype, etype, dtype], (feat_src, feat_dst))
                out_tensor = out[dtype].squeeze(1)
            except:
                out = self.gat2[etype](g[stype, etype, dtype], (h1[stype][:, :64], h1[dtype][:, :64]))
                out_tensor = out.squeeze(1)

            # èšåˆåˆ°ç›®æ ‡èŠ‚ç‚¹ç±»å‹
            if dtype not in h2:
                h2[dtype] = []
            h2[dtype].append(out_tensor)

        # æœ€ç»ˆç‰¹å¾èšåˆ
        final_embeds = {}
        for ntype in g.ntypes:
            if ntype in h2 and len(h2[ntype]) > 0:
                final_embeds[ntype] = torch.stack(h2[ntype], dim=0).sum(dim=0)
            else:
                final_embeds[ntype] = h1[ntype][:, :64]  # ç¡®ä¿64ç»´

            # L2å½’ä¸€åŒ–
            final_embeds[ntype] = F.normalize(final_embeds[ntype], p=2, dim=1)

        return final_embeds


# ===================== Whoisè¡¥å…¨æ ¸å¿ƒç±»ï¼ˆä¿®å¤èŠ‚ç‚¹-ç‰¹å¾åŒ¹é…ï¼‰ =====================
class WhoisLinkPredCompleter:
    def __init__(self,
                 source_csv: str,
                 merged_json: str,
                 entity_map: str,
                 model_path: str,
                 seed_graph_path: str,
                 output_path: str = "completed_whois.csv",
                 updated_graph_path: str = "updated_ipv6_graph.bin"):
        # è·¯å¾„é…ç½®
        self.source_csv = source_csv
        self.merged_json = merged_json
        self.entity_map = entity_map
        self.model_path = model_path
        self.seed_graph_path = seed_graph_path
        self.output_path = output_path
        self.updated_graph_path = updated_graph_path

        # æ•°æ®å­˜å‚¨
        self.source_df = None
        self.merged_df = None
        self.combined_df = None
        self.seed_graph = None
        self.temp_graph = None
        self.updated_graph = None

        # å®ä½“æ˜ å°„
        self.ent_val_to_id = defaultdict(dict)
        self.ent_id_to_val = defaultdict(dict)
        self.next_ent_id = defaultdict(int)

        # æ¨¡å‹ç›¸å…³
        self.han_model = None
        self.seed_embeds = {}
        self.non_seed_embeds = {}
        self.candidate_pool = defaultdict(list)

        # éç§å­å›¾çš„èŠ‚ç‚¹IDæ˜ å°„ï¼ˆå…³é”®ï¼šè®°å½•æ‰€æœ‰èŠ‚ç‚¹IDï¼‰
        self.prefix_id_map = {}  # prefix_str -> node_id
        self.other_id_maps = {
            'Country': {}, 'Keyword': {}, 'Mnt': {},
            'Netname': {}, 'Status': {}
        }

    def load_seed_resources(self):
        """åŠ è½½ç§å­èµ„æº"""
        print("ğŸ” åŠ è½½ç§å­èµ„æº...")

        # 1. åŠ è½½å®ä½“æ˜ å°„è¡¨
        mapping_df = pd.read_csv(self.entity_map)
        for _, row in mapping_df.iterrows():
            etype = row['entity_type']
            eval = row['entity_value']
            eid = row['entity_id']
            self.ent_val_to_id[etype][eval] = eid
            self.ent_id_to_val[etype][eid] = eval
        # åˆå§‹åŒ–éç§å­å®ä½“ID
        for etype in self.ent_val_to_id:
            self.next_ent_id[etype] = max(self.ent_val_to_id[etype].values()) + 1 if self.ent_val_to_id[etype] else 0
        print(f"âœ… å®ä½“æ˜ å°„åŠ è½½å®Œæˆï¼š{len(mapping_df)}ä¸ªç§å­å®ä½“")

        # 2. åŠ è½½ç§å­å¼‚æ„å›¾
        graphs, _ = dgl.load_graphs(self.seed_graph_path)
        self.seed_graph = graphs[0].to(DEVICE)
        print(
            f"âœ… ç§å­å¼‚æ„å›¾åŠ è½½å®Œæˆï¼šèŠ‚ç‚¹ç±»å‹={self.seed_graph.ntypes}ï¼Œè¾¹ç±»å‹={[e[1] for e in self.seed_graph.canonical_etypes]}")

        # 3. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        in_feats_dict = {ntype: self.seed_graph.nodes[ntype].data['feat'].shape[1] for ntype in self.seed_graph.ntypes}
        self.han_model = HANLinkPredModel(
            g=self.seed_graph,
            in_feats_dict=in_feats_dict
        ).to(DEVICE)

        # éä¸¥æ ¼åŠ è½½æƒé‡
        state_dict = torch.load(self.model_path, map_location=DEVICE)
        self.han_model.load_state_dict(state_dict, strict=False)

        # å›ºå®šæ‰€æœ‰å‚æ•°
        for param in self.han_model.parameters():
            param.requires_grad = False
        self.han_model.eval()
        print(f"âœ… è®­ç»ƒæ¨¡å‹åŠ è½½å®Œæˆï¼ˆæ ¸å¿ƒå±‚å‚æ•°åŒ¹é…ï¼‰")

        # 4. ç”Ÿæˆç§å­å®ä½“åµŒå…¥
        with torch.no_grad():
            self.seed_embeds = self.han_model()
        print(f"âœ… ç§å­åµŒå…¥ç”Ÿæˆå®Œæˆï¼š{[f'{k}:{v.shape}' for k, v in self.seed_embeds.items()]}")

        # 5. æ„å»ºå€™é€‰å®ä½“æ± 
        self._build_candidate_pool()

    def _build_candidate_pool(self):
        """æ„å»ºå€™é€‰å®ä½“æ± ï¼ˆä¿®å¤è¾¹ç±»å‹éå†ï¼‰"""
        print("ğŸ” æ„å»ºå€™é€‰å®ä½“æ± ...")
        candidate_types = ['Mnt', 'Netname', 'Country', 'Status']

        for etype in candidate_types:
            # æ‰¾åˆ°æ‰€æœ‰æŒ‡å‘è¯¥å®ä½“ç±»å‹çš„è¾¹
            edge_types = [e for e in self.seed_graph.canonical_etypes if e[2] == etype]
            total_counts = Counter()

            for stype, edge_type, dtype in edge_types:
                if dtype != etype:
                    continue
                try:
                    # è·å–è¾¹çš„ç›®æ ‡èŠ‚ç‚¹ID
                    _, dst_ids = self.seed_graph.edges(etype=edge_type)
                    total_counts.update(dst_ids.cpu().numpy())
                except:
                    continue

            # ç­›é€‰é«˜é¢‘å®ä½“
            for eid, freq in total_counts.most_common():
                if freq >= MIN_CANDIDATE_FREQ:
                    eval = self.ent_id_to_val[etype].get(eid)
                    if eval:
                        self.candidate_pool[etype].append((eval, eid))

            print(f"   - {etype}ï¼š{len(self.candidate_pool[etype])}ä¸ªå€™é€‰å®ä½“ï¼ˆé¢‘æ¬¡â‰¥{MIN_CANDIDATE_FREQ}ï¼‰")

    def load_and_preprocess_data(self):
        """åŠ è½½å¹¶é¢„å¤„ç†éç§å­æ•°æ®"""
        print("\nğŸ“Š åŠ è½½å¹¶é¢„å¤„ç†éç§å­æ•°æ®...")

        # 1. åŠ è½½æºæ•°æ®
        self.source_df = pd.read_csv(self.source_csv)
        with open(self.merged_json, 'r', encoding='utf-8') as f:
            merged_dict = json.load(f)
        self.merged_df = pd.DataFrame.from_dict(merged_dict, orient='index').reset_index()
        self.merged_df.rename(columns={'index': 'inet6num'}, inplace=True)

        # 2. æ•°æ®åˆå¹¶
        self.combined_df = pd.merge(
            self.source_df,
            self.merged_df,
            on='inet6num',
            how='outer',
            suffixes=('', '_merged')
        )

        # 3. æ ‡è®°ç§å­/éç§å­å‰ç¼€
        seed_prefixes = set(self.ent_val_to_id.get('Prefix', {}).keys())
        self.combined_df['is_seed'] = self.combined_df['inet6num'].isin(seed_prefixes)
        self.non_seed_df = self.combined_df[~self.combined_df['is_seed']].copy().reset_index(drop=True)

        # è¿‡æ»¤ç©ºçš„prefix
        self.non_seed_df = self.non_seed_df[~self.non_seed_df['inet6num'].isna()].reset_index(drop=True)

        print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼šæ€»è®°å½•={len(self.combined_df)}ï¼Œéç§å­={len(self.non_seed_df)}")

        # 4. åŸºç¡€é¢„å¤„ç†
        self._basic_preprocess()

    def _basic_preprocess(self):
        """åŸºç¡€é¢„å¤„ç†"""
        # æ ‡å‡†åŒ–å­—æ®µæ ¼å¼
        str_fields = ['netname', 'country', 'mnt-by', 'status', 'descr']
        for field in str_fields:
            if field in self.combined_df.columns:
                self.combined_df[field] = self.combined_df[field].astype(str).str.strip().str.upper()
                self.combined_df.loc[self.combined_df[field].isin(['NAN', 'NaN', 'nan']), field] = np.nan

        # æ´¾ç”Ÿdescr_cleaned
        if 'descr_cleaned' not in self.combined_df.columns:
            self.combined_df['descr_cleaned'] = np.nan

        mask = self.combined_df['descr_cleaned'].isna() & ~self.combined_df['descr'].isna()
        if mask.sum() > 0:
            def clean_descr(s):
                tokens = re.split(r'[\s-]+', s.lower())
                return ' '.join([t for t in tokens if t.isalpha() and t not in STOP_WORDS and len(t) >= 3]).upper()

            self.combined_df.loc[mask, 'descr_cleaned'] = self.combined_df.loc[mask, 'descr'].apply(clean_descr)

        # æ ‡è®°å¾…è¡¥å…¨å­—æ®µ
        self.combined_df['to_complete'] = ''
        total_missing = 0
        for field in ['netname', 'country', 'mnt-by', 'status']:
            if field in self.combined_df.columns:
                mask = self.combined_df[field].isna() & ~self.combined_df['is_seed']
                self.combined_df.loc[mask, 'to_complete'] += f'{field},'
                total_missing += mask.sum()

        print(f"âœ… å¾…è¡¥å…¨å­—æ®µæ ‡è®°å®Œæˆï¼š{total_missing}æ¡è®°å½•éœ€è¡¥å…¨")

    def build_non_seed_graph(self):
        """æ„å»ºéç§å­ä¸´æ—¶å¼‚æ„å›¾ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šèŠ‚ç‚¹-ç‰¹å¾åŒ¹é…ï¼‰"""
        print("\nğŸŒ æ„å»ºéç§å­ä¸´æ—¶å¼‚æ„å›¾...")

        # é‡ç½®IDæ˜ å°„
        self.prefix_id_map.clear()
        for ntype in self.other_id_maps:
            self.other_id_maps[ntype].clear()

        # 1. ç¬¬ä¸€æ­¥ï¼šåˆ†é…æ‰€æœ‰èŠ‚ç‚¹IDï¼ˆç¡®ä¿IDè¿ç»­ï¼‰
        print("   - åˆ†é…éç§å­èŠ‚ç‚¹ID...")
        for idx, row in self.non_seed_df.iterrows():
            prefix = row['inet6num']
            if pd.isna(prefix):
                continue

            # åˆ†é…Prefix ID
            if prefix not in self.prefix_id_map:
                self.prefix_id_map[prefix] = len(self.prefix_id_map)

            # åˆ†é…å…³è”å®ä½“IDï¼ˆä»…åˆ†é…IDï¼Œæš‚ä¸æ·»åŠ è¾¹ï¼‰
            self._assign_other_entity_ids(row)

        # 2. ç¬¬äºŒæ­¥ï¼šæ”¶é›†æ‰€æœ‰è¾¹
        print("   - æ”¶é›†è¾¹ä¿¡æ¯...")
        graph_data = defaultdict(list)
        for idx, row in self.non_seed_df.iterrows():
            prefix = row['inet6num']
            if pd.isna(prefix) or prefix not in self.prefix_id_map:
                continue

            prefix_id = self.prefix_id_map[prefix]
            self._add_edges_for_prefix(prefix_id, row, graph_data)

        # 3. ç¬¬ä¸‰æ­¥ï¼šæ„å»ºå¼‚æ„å›¾ï¼ˆå…ˆæŒ‡å®šèŠ‚ç‚¹æ•°é‡ï¼Œå†æ·»åŠ è¾¹ï¼‰
        print("   - åˆ›å»ºå¼‚æ„å›¾...")
        # å®šä¹‰èŠ‚ç‚¹æ•°é‡
        num_nodes_dict = {
            'Prefix': len(self.prefix_id_map),
            'Country': len(self.other_id_maps['Country']),
            'Keyword': len(self.other_id_maps['Keyword']),
            'Mnt': len(self.other_id_maps['Mnt']),
            'Netname': len(self.other_id_maps['Netname']),
            'Status': len(self.other_id_maps['Status'])
        }

        # è¿‡æ»¤ç©ºè¾¹
        filtered_graph_data = {}
        for (stype, etype, dtype), edges in graph_data.items():
            if edges:
                src_ids = torch.tensor([e[0] for e in edges], dtype=torch.long, device=DEVICE)
                dst_ids = torch.tensor([e[1] for e in edges], dtype=torch.long, device=DEVICE)
                filtered_graph_data[(stype, etype, dtype)] = (src_ids, dst_ids)

        # åˆ›å»ºå¼‚æ„å›¾ï¼ˆæŒ‡å®šèŠ‚ç‚¹æ•°é‡ï¼‰
        self.temp_graph = dgl.heterograph(filtered_graph_data, num_nodes_dict=num_nodes_dict, device=DEVICE)

        # 4. ç¬¬å››æ­¥ï¼šæ·»åŠ èŠ‚ç‚¹ç‰¹å¾ï¼ˆå…³é”®ï¼šç»´åº¦ä¸¥æ ¼åŒ¹é…èŠ‚ç‚¹æ•°ï¼‰
        print("   - æ·»åŠ èŠ‚ç‚¹ç‰¹å¾...")
        self._add_node_features_to_temp_graph()

        print(f"âœ… éç§å­ä¸´æ—¶å¼‚æ„å›¾æ„å»ºå®Œæˆï¼š")
        print(f"   - èŠ‚ç‚¹ç±»å‹ï¼š{self.temp_graph.ntypes}")
        print(f"   - è¾¹ç±»å‹ï¼š{[e[1] for e in self.temp_graph.canonical_etypes]}")
        print(f"   - PrefixèŠ‚ç‚¹æ•°ï¼š{self.temp_graph.num_nodes('Prefix')}")

    def _assign_other_entity_ids(self, row):
        """åªä¸ºå®ä½“åˆ†é…IDï¼ˆç¡®ä¿IDè¿ç»­ï¼‰"""
        ent_mapping = {
            'mnt-by': 'Mnt',
            'netname': 'Netname',
            'country': 'Country',
            'status': 'Status',
            'descr_cleaned': 'Keyword'
        }

        for field, ntype in ent_mapping.items():
            if field not in row:
                continue
            val = row[field]
            if pd.isna(val) or val in ['NAN', 'NaN', 'nan', '']:
                continue

            if val not in self.other_id_maps[ntype]:
                self.other_id_maps[ntype][val] = len(self.other_id_maps[ntype])

    def _add_edges_for_prefix(self, prefix_id, row, graph_data):
        """ä¸ºå•ä¸ªPrefixæ·»åŠ è¾¹"""
        ent_mapping = {
            'mnt-by': ('Mnt', 'prefix_to_mnt'),
            'netname': ('Netname', 'prefix_to_netname'),
            'country': ('Country', 'prefix_to_country'),
            'status': ('Status', 'prefix_to_status'),
            'descr_cleaned': ('Keyword', 'prefix_to_keyword')
        }

        for field, (ntype, edge_type) in ent_mapping.items():
            if field not in row:
                continue
            val = row[field]
            if pd.isna(val) or val in ['NAN', 'NaN', 'nan', '']:
                continue

            if val in self.other_id_maps[ntype]:
                entity_id = self.other_id_maps[ntype][val]
                graph_data[('Prefix', edge_type, ntype)].append((prefix_id, entity_id))

    def _add_node_features_to_temp_graph(self):
        """æ·»åŠ èŠ‚ç‚¹ç‰¹å¾ï¼ˆç¡®ä¿ç»´åº¦åŒ¹é…ï¼‰"""
        # 1. æ·»åŠ Prefixç‰¹å¾ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šç»´åº¦ä¸¥æ ¼åŒ¹é…ï¼‰
        if self.temp_graph.num_nodes('Prefix') > 0:
            prefix_feats = torch.zeros((self.temp_graph.num_nodes('Prefix'), 33), device=DEVICE)

            # éå†æ‰€æœ‰PrefixèŠ‚ç‚¹ï¼Œå¡«å……ç‰¹å¾
            for prefix, pid in self.prefix_id_map.items():
                if pid >= self.temp_graph.num_nodes('Prefix'):
                    continue  # è·³è¿‡è¶…å‡ºèŒƒå›´çš„ID
                # æ‰¾åˆ°å¯¹åº”çš„è¡Œ
                row_mask = self.non_seed_df['inet6num'] == prefix
                if row_mask.sum() > 0:
                    row = self.non_seed_df[row_mask].iloc[0]
                    prefix_feats[pid] = self._generate_prefix_feat(prefix, row)

            self.temp_graph.nodes['Prefix'].data['feat'] = prefix_feats

        # 2. æ·»åŠ å…¶ä»–èŠ‚ç‚¹ç‰¹å¾
        for ntype in ['Country', 'Mnt', 'Netname', 'Status', 'Keyword']:
            num_nodes = self.temp_graph.num_nodes(ntype)
            if num_nodes == 0:
                continue

            feats = torch.zeros((num_nodes, 2), device=DEVICE)
            for val, eid in self.other_id_maps[ntype].items():
                if eid >= num_nodes:
                    continue

                # å¤ç”¨ç§å­ç‰¹å¾
                if val in self.ent_val_to_id.get(ntype, {}):
                    seed_eid = self.ent_val_to_id[ntype][val]
                    if seed_eid < self.seed_graph.num_nodes(ntype):
                        feats[eid] = self.seed_graph.nodes[ntype].data['feat'][seed_eid]
                else:
                    feats[eid] = torch.randn(2, device=DEVICE)

            self.temp_graph.nodes[ntype].data['feat'] = feats

    def _generate_prefix_feat(self, prefix: str, row: pd.Series) -> torch.Tensor:
        """ç”Ÿæˆ33ç»´Prefixç‰¹å¾"""
        feat = []

        # 1. å‰ç¼€é•¿åº¦ï¼ˆ1ç»´ï¼‰
        try:
            plen = int(prefix.split('/')[-1]) if '/' in str(prefix) else 0
            feat.append(torch.tensor([plen / 128.0], dtype=torch.float32, device=DEVICE))
        except:
            feat.append(torch.tensor([0.0], dtype=torch.float32, device=DEVICE))

        # 2. IPv6ç‰ˆæœ¬æ ‡è¯†ï¼ˆ1ç»´ï¼‰
        feat.append(torch.tensor([1.0], dtype=torch.float32, device=DEVICE))

        # 3. ç½‘ç»œç±»å‹ç‹¬çƒ­ç¼–ç ï¼ˆ8ç»´ï¼‰
        net_type = torch.zeros(8, dtype=torch.float32, device=DEVICE)
        try:
            plen = int(prefix.split('/')[-1]) if '/' in str(prefix) else 0
            if plen <= 12:
                net_type[0] = 1.0
            elif plen <= 24:
                net_type[1] = 1.0
            elif plen <= 32:
                net_type[2] = 1.0
            elif plen <= 48:
                net_type[3] = 1.0
            elif plen <= 64:
                net_type[4] = 1.0
            elif plen <= 80:
                net_type[5] = 1.0
            elif plen <= 96:
                net_type[6] = 1.0
            else:
                net_type[7] = 1.0
        except:
            net_type[7] = 1.0
        feat.append(net_type)

        # 4. å›½å®¶ç‹¬çƒ­ç¼–ç ï¼ˆ10ç»´ï¼‰
        top_countries = ['US', 'CN', 'JP', 'DE', 'UK', 'FR', 'KR', 'CA', 'AU', 'IN']
        country_onehot = torch.zeros(10, dtype=torch.float32, device=DEVICE)
        if not pd.isna(row.get('country')) and row['country'] in top_countries:
            country_onehot[top_countries.index(row['country'])] = 1.0
        feat.append(country_onehot)

        # 5. å…³é”®å­—è¯è¢‹ï¼ˆ10ç»´ï¼‰
        top_keywords = ['INTERNET', 'SERVICE', 'PROVIDER', 'NETWORK', 'COMMUNICATION',
                        'TECHNOLOGY', 'CORPORATION', 'ORGANIZATION', 'GOVERNMENT', 'EDUCATION']
        keyword_bow = torch.zeros(10, dtype=torch.float32, device=DEVICE)
        if not pd.isna(row.get('descr_cleaned')):
            desc = row['descr_cleaned']
            for i, kw in enumerate(top_keywords):
                if kw in desc:
                    keyword_bow[i] = 1.0
        feat.append(keyword_bow)

        # 6. çŠ¶æ€ç‹¬çƒ­ç¼–ç ï¼ˆ3ç»´ï¼‰
        status_onehot = torch.zeros(3, dtype=torch.float32, device=DEVICE)
        if not pd.isna(row.get('status')):
            status = row['status']
            if 'ALLOCATED' in status:
                status_onehot[0] = 1.0
            elif 'ASSIGNED' in status:
                status_onehot[1] = 1.0
            elif 'RESERVED' in status:
                status_onehot[2] = 1.0
        feat.append(status_onehot)

        return torch.cat(feat, dim=0)

    def predict_missing_edges(self):
        """é“¾è·¯é¢„æµ‹è¡¥å…¨ç¼ºå¤±å®ä½“"""
        print("\nğŸ¯ é“¾è·¯é¢„æµ‹è¡¥å…¨ç¼ºå¤±å®ä½“ï¼ˆé˜ˆå€¼=0.7ï¼‰...")

        # æ£€æŸ¥ä¸´æ—¶å›¾æ˜¯å¦ä¸ºç©º
        if self.temp_graph is None or self.temp_graph.num_nodes('Prefix') == 0:
            print("âš ï¸ éç§å­å›¾ä¸ºç©ºï¼Œè·³è¿‡è¡¥å…¨")
            return

        # ç”Ÿæˆéç§å­PrefixåµŒå…¥
        with torch.no_grad():
            self.non_seed_embeds = self.han_model(input_g=self.temp_graph)

        # æ£€æŸ¥PrefixåµŒå…¥
        if 'Prefix' not in self.non_seed_embeds or len(self.non_seed_embeds['Prefix']) == 0:
            print("âš ï¸ æ— PrefixåµŒå…¥ï¼Œè·³è¿‡è¡¥å…¨")
            return

        prefix_embeds = self.non_seed_embeds['Prefix']
        print(f"âœ… éç§å­PrefixåµŒå…¥ç”Ÿæˆå®Œæˆï¼š{prefix_embeds.shape}ï¼ˆ64ç»´ï¼‰")

        # å¾…è¡¥å…¨å­—æ®µæ˜ å°„
        complete_mapping = {
            'mnt-by': ('Mnt', 'prefix_to_mnt'),
            'netname': ('Netname', 'prefix_to_netname'),
            'country': ('Country', 'prefix_to_country'),
            'status': ('Status', 'prefix_to_status')
        }

        # é€ä¸ªå­—æ®µè¡¥å…¨
        total_completed = 0
        for field, (etype, edge_type) in complete_mapping.items():
            if field not in self.combined_df.columns:
                continue
            # è·³è¿‡æ— å€™é€‰å®ä½“çš„å­—æ®µ
            if etype not in self.candidate_pool or len(self.candidate_pool[etype]) == 0:
                print(f"   - {field}ï¼š0ä¸ªå€™é€‰å®ä½“ï¼Œè·³è¿‡")
                continue
            completed = self._complete_single_field(field, etype, edge_type, prefix_embeds)
            total_completed += completed
            print(f"   - {field}ï¼šè¡¥å…¨{completed}æ¡ï¼ˆé˜ˆå€¼=0.7ï¼‰")

        print(f"âœ… é“¾è·¯é¢„æµ‹è¡¥å…¨å®Œæˆï¼šæ€»è®¡è¡¥å…¨{total_completed}æ¡")

    def _complete_single_field(self, field: str, etype: str, edge_type: str, prefix_embeds: torch.Tensor) -> int:
        """è¡¥å…¨å•ä¸ªå­—æ®µ"""
        # ç­›é€‰å¾…è¡¥å…¨è®°å½•
        mask = self.combined_df[field].isna() & ~self.combined_df['is_seed']
        if not mask.any():
            return 0

        # å€™é€‰å®ä½“
        candidate_vals = [c[0] for c in self.candidate_pool[etype]]
        candidate_ids = [c[1] for c in self.candidate_pool[etype]]

        # æ£€æŸ¥ç§å­åµŒå…¥
        if etype not in self.seed_embeds or len(candidate_ids) == 0:
            return 0

        candidate_embeds = self.seed_embeds[etype][candidate_ids].to(DEVICE)
        completed = 0

        # éå†å¾…è¡¥å…¨è®°å½•
        for idx in self.combined_df[mask].index:
            row = self.combined_df.iloc[idx]
            prefix = row['inet6num']

            # è·å–Prefix ID
            if prefix not in self.prefix_id_map:
                continue
            prefix_id = self.prefix_id_map[prefix]
            if prefix_id >= len(prefix_embeds):
                continue

            # è®¡ç®—å…³è”æ¦‚ç‡ï¼ˆå…¬å¼9ï¼šÏƒ(h_uÂ·h_v)ï¼‰
            p_embed = prefix_embeds[prefix_id:prefix_id + 1]  # [1, 64]
            dot_products = torch.matmul(p_embed, candidate_embeds.T).squeeze(0)
            probs = torch.sigmoid(dot_products).cpu().numpy()

            # é€‰æ‹©æœ€ä¼˜å€™é€‰
            max_idx = np.argmax(probs)
            max_prob = probs[max_idx]

            if max_prob > CONFIDENCE_THRESHOLD:
                best_candidate = candidate_vals[max_idx]
                # ä¸€è‡´æ€§æ ¡éªŒ
                if self._consistency_check(row, field, best_candidate):
                    self.combined_df.at[idx, field] = best_candidate
                    self.combined_df.at[idx, f'{field}_confidence'] = float(max_prob)
                    completed += 1

        return completed

    def _consistency_check(self, row: pd.Series, field: str, candidate: str) -> bool:
        """ä¸€è‡´æ€§æ ¡éªŒ"""
        # Mntä¸Countryå†²çªæ ¡éªŒ
        if field == 'mnt-by':
            country = row.get('country')
            if not pd.isna(country) and country != '':
                country_code = re.findall(r'^[A-Z]{2}', candidate)
                if country_code and country_code[0] != country:
                    return False

        # Statusåˆç†æ€§æ ¡éªŒ
        if field == 'status':
            valid_status = ['ALLOCATED', 'ASSIGNED', 'RESERVED']
            if not any(vs in candidate for vs in valid_status):
                return False

        return True

    def update_hetero_graph(self):
        """æ›´æ–°å¼‚æ„å›¾"""
        print("\nğŸ”„ æ›´æ–°éç§å­å¼‚æ„å›¾...")
        if self.temp_graph is None:
            print("âš ï¸ æ— ä¸´æ—¶å›¾ï¼Œè·³è¿‡æ›´æ–°")
            return

        try:
            # åˆå¹¶ç§å­å›¾å’Œéç§å­å›¾
            self.updated_graph = dgl.merge([self.seed_graph, self.temp_graph])
            dgl.save_graphs(self.updated_graph_path, [self.updated_graph])
            print(f"âœ… å¼‚æ„å›¾æ›´æ–°å®Œæˆï¼Œä¿å­˜è‡³ï¼š{self.updated_graph_path}")
        except Exception as e:
            print(f"âš ï¸ åˆå¹¶å›¾å¤±è´¥ï¼š{e}")

    def save_results(self):
        """ä¿å­˜è¡¥å…¨ç»“æœ"""
        # ç»Ÿè®¡è¡¥å…¨æ•ˆæœ
        stats = []
        for field in ['netname', 'country', 'mnt-by', 'status']:
            if field not in self.combined_df.columns:
                continue
            total = len(self.non_seed_df)
            missing_before = self.non_seed_df[field].isna().sum()
            missing_after = self.combined_df[~self.combined_df['is_seed']][field].isna().sum()
            completed = missing_before - missing_after
            completion_rate = (completed / total) * 100 if total > 0 else 0
            stats.append({
                'å­—æ®µ': field,
                'éç§å­æ€»æ•°': total,
                'è¡¥å…¨æ•°': completed,
                'è¡¥å…¨ç‡': f'{completion_rate:.1f}%'
            })

        # æ‰“å°ç»Ÿè®¡
        print("\nğŸ“ˆ è¡¥å…¨æ•ˆæœç»Ÿè®¡ï¼š")
        print(pd.DataFrame(stats).to_string(index=False))

        # ä¿å­˜ç»“æœ
        try:
            self.combined_df.to_csv(self.output_path, index=False, encoding='utf-8')
            print(f"âœ… è¡¥å…¨ç»“æœå·²ä¿å­˜è‡³ï¼š{self.output_path}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç»“æœå¤±è´¥ï¼š{e}")

    def run(self):
        """æ‰§è¡Œå®Œæ•´æµç¨‹"""
        print("=" * 80)
        print("6HAN Whoisä¿¡æ¯è¡¥å…¨æµç¨‹ï¼ˆåŸºäºé“¾è·¯é¢„æµ‹ï¼‰")
        print("=" * 80)

        try:
            self.load_seed_resources()
            self.load_and_preprocess_data()
            self.build_non_seed_graph()
            self.predict_missing_edges()
            self.update_hetero_graph()
            self.save_results()

            print("\nğŸ‰ è¡¥å…¨æµç¨‹å…¨éƒ¨å®Œæˆï¼")
        except Exception as e:
            import traceback
            traceback.print_exc()
            # ä¿å­˜éƒ¨åˆ†ç»“æœ
            if self.combined_df is not None:
                self.combined_df.to_csv(f"partial_{self.output_path}", index=False, encoding='utf-8')
                print(f"âš ï¸ æµç¨‹ä¸­æ–­ï¼Œå·²ä¿å­˜éƒ¨åˆ†ç»“æœè‡³ï¼špartial_{self.output_path}")
            raise RuntimeError(f"è¡¥å…¨æµç¨‹å¤±è´¥ï¼š{str(e)}")


# ===================== æ‰§è¡Œå…¥å£ =====================
if __name__ == "__main__":
    # é…ç½®è·¯å¾„ï¼ˆæ ¹æ®å®é™…ä¿®æ”¹ï¼‰
    config = {
        "source_csv": "Data/parsed_whois.csv",
        "merged_json": "Data/merged_whois.json",
        "entity_map": "Data/entity_mapping.csv",
        "model_path": "Data/6han_model.pth",
        "seed_graph_path": "Data/ipv6_hetero_graph.bin",
        "output_path": "completed_whois.csv",
        "updated_graph_path": "updated_ipv6_graph.bin"
    }

    # æ‰§è¡Œè¡¥å…¨
    completer = WhoisLinkPredCompleter(**config)
    completer.run()